{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic implementation using jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%s\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-01-09 12:51:34,414] [INFO] [<module>] [12] : Starting ...\n"
     ]
    }
   ],
   "source": [
    "import logging, numpy as np\n",
    "from scipy import signal\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import platform\n",
    "import dlib\n",
    "\n",
    "FORMAT = '[%(asctime)s] [%(levelname)s] [%(funcName)s] [%(lineno)d] : %(message)s'\n",
    "logging.basicConfig(format=FORMAT, level=logging.INFO)\n",
    "\n",
    "logging.info(\"Starting ...\")\n",
    "if platform.system() == \"Windows\":\n",
    "    seperator = \"\\\\\"\n",
    "else:\n",
    "    seperator = \"/\"\n",
    "\n",
    "dir = \"perry-all-2\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rotate image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%s\n"
    }
   },
   "outputs": [],
   "source": [
    "def rotate_image(mat, angle):\n",
    "    \"\"\"\n",
    "    Rotates an image (angle in degrees) and expands image to avoid cropping\n",
    "    \"\"\"\n",
    "\n",
    "    height, width = mat.shape[:2]  # image shape has 3 dimensions\n",
    "    image_center = (\n",
    "        width / 2,\n",
    "        height / 2)  # getRotationMatrix2D needs coordinates in reverse order (width, height) compared to shape\n",
    "\n",
    "    rotation_mat = cv2.getRotationMatrix2D(image_center, angle, 1.)\n",
    "\n",
    "    # rotation calculates the cos and sin, taking absolutes of those.\n",
    "    abs_cos = abs(rotation_mat[0, 0])\n",
    "    abs_sin = abs(rotation_mat[0, 1])\n",
    "\n",
    "    # find the new width and height bounds\n",
    "    bound_w = int(height * abs_sin + width * abs_cos)\n",
    "    bound_h = int(height * abs_cos + width * abs_sin)\n",
    "\n",
    "    # subtract old image center (bringing image back to origo) and adding the new image center coordinates\n",
    "    rotation_mat[0, 2] += bound_w / 2 - image_center[0]\n",
    "    rotation_mat[1, 2] += bound_h / 2 - image_center[1]\n",
    "\n",
    "    # rotate image with the new bounds and translated rotation matrix\n",
    "    rotated_mat = cv2.warpAffine(mat, rotation_mat, (bound_w, bound_h))\n",
    "    return rotated_mat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse roi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%s\n"
    }
   },
   "outputs": [],
   "source": [
    "cnn_face_detector = dlib.cnn_face_detection_model_v1(\"./models/mmod_human_face_detector.dat\")\n",
    "def parse_roi(img,method= \"dlib_hog\"):\n",
    "    \"\"\"\n",
    "    Upon receiving an image, finds a face (if exists) and writes it as an image\n",
    "    :param image: the image to be parsed\n",
    "    \"\"\"\n",
    "    # The 1 in the second argument indicates that we should upsample the image\n",
    "    # 1 time.  This will make everything bigger and allow us to detect more\n",
    "    # faces.\n",
    "    if method == \"dlib_hog\":\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        hogFaceDetector = dlib.get_frontal_face_detector()\n",
    "        faces = hogFaceDetector(gray, 1)\n",
    "        if len(faces) ==0:\n",
    "            logging.warning(\"No face detected in image\")\n",
    "        for rect in faces:\n",
    "            x = rect.left()\n",
    "            y = rect.top()\n",
    "            w = rect.right() - x\n",
    "            h = rect.bottom() - y\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            roi_color = img[y:y + h, x:x + w]\n",
    "            cv2.imwrite(\"faces_detected.jpg\",roi_color)\n",
    "#             rgb = cv2.cvtColor(roi_color, cv2.COLOR_BGR2RGB)\n",
    "#             plt.imshow(rgb)\n",
    "#             plt.show()\n",
    "    else:\n",
    "        dets = cnn_face_detector(img, 1)\n",
    "        for d in dets:\n",
    "            flag_face_detected = True\n",
    "            crop = img[d.rect.top():d.rect.bottom(), d.rect.left():d.rect.right()]\n",
    "            cv2.imwrite(\"faces_detected.jpg\",crop)\n",
    "        if not flag_face_detected:\n",
    "                logging.warning(\"No face detected in image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%s\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_result(greens, reds, blues, x_value, title=\"\"):\n",
    "    logging.info(\"Plotting results ...\" + title)\n",
    "    x_value = x_value.reshape(1, x_value.shape[0])\n",
    "    r = reds.tolist()[0]\n",
    "    b = blues.tolist()[0]\n",
    "    g = greens.tolist()[0]\n",
    "    x = x_value.tolist()[0]\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(x, g, color=\"green\")\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(x, r, color=\"red\")\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(x, b, color=\"blue\")\n",
    "    plt.show()\n",
    "    logging.info(\"Showing result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%s\n"
    }
   },
   "outputs": [],
   "source": [
    "def parse_RGB(image, vidcap, greens, reds, blues, frame_number):\n",
    "    \"\"\"\n",
    "    Parses an image to its RGB channels\n",
    "    :param image: the image to be parsed\n",
    "    :param vidcap:\n",
    "    :param greens: array containing green channel values\n",
    "    :param blues: array containing blue channel values\n",
    "    :param reds: array containing red channel values\n",
    "    :param frame_number - is the number of the frame of the video.\n",
    "    :return: a flag indicating if there is a next image, and the next image\n",
    "    \"\"\"\n",
    "    blue, green, red = cv2.split(image)\n",
    "    greens[0, frame_number] = np.mean(green)\n",
    "    blues[0, frame_number] = np.mean(blue)\n",
    "    reds[0, frame_number] = np.mean(red)\n",
    "    success, image = vidcap.read()\n",
    "    return success, image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### read video frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%s\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-01-09 12:51:34,517] [INFO] [<module>] [5] : Working on video ..\\dataset\\good_sync\\perry-all-2\\test.mp4\n",
      "[2021-01-09 12:51:34,578] [INFO] [<module>] [16] : Parsing images ...\n",
      "[2021-01-09 12:51:35,501] [INFO] [<module>] [23] : frame number: 1\n",
      "[2021-01-09 12:51:36,428] [INFO] [<module>] [23] : frame number: 2\n",
      "[2021-01-09 12:51:37,349] [INFO] [<module>] [23] : frame number: 3\n",
      "[2021-01-09 12:51:38,300] [INFO] [<module>] [23] : frame number: 4\n",
      "[2021-01-09 12:51:39,327] [INFO] [<module>] [23] : frame number: 5\n",
      "[2021-01-09 12:51:40,333] [INFO] [<module>] [23] : frame number: 6\n",
      "[2021-01-09 12:51:41,314] [INFO] [<module>] [23] : frame number: 7\n",
      "[2021-01-09 12:51:42,275] [INFO] [<module>] [23] : frame number: 8\n",
      "[2021-01-09 12:51:43,289] [INFO] [<module>] [23] : frame number: 9\n",
      "[2021-01-09 12:51:44,175] [INFO] [<module>] [23] : frame number: 10\n",
      "[2021-01-09 12:51:45,071] [INFO] [<module>] [23] : frame number: 11\n",
      "[2021-01-09 12:51:45,976] [INFO] [<module>] [23] : frame number: 12\n",
      "[2021-01-09 12:51:46,969] [INFO] [<module>] [23] : frame number: 13\n",
      "[2021-01-09 12:51:48,130] [INFO] [<module>] [23] : frame number: 14\n",
      "[2021-01-09 12:51:49,313] [INFO] [<module>] [23] : frame number: 15\n",
      "[2021-01-09 12:51:50,351] [INFO] [<module>] [23] : frame number: 16\n",
      "[2021-01-09 12:51:51,317] [INFO] [<module>] [23] : frame number: 17\n",
      "[2021-01-09 12:51:52,243] [INFO] [<module>] [23] : frame number: 18\n",
      "[2021-01-09 12:51:53,650] [INFO] [<module>] [23] : frame number: 19\n",
      "[2021-01-09 12:51:54,814] [INFO] [<module>] [23] : frame number: 20\n",
      "[2021-01-09 12:51:55,871] [INFO] [<module>] [23] : frame number: 21\n",
      "[2021-01-09 12:51:56,824] [INFO] [<module>] [23] : frame number: 22\n",
      "[2021-01-09 12:51:58,108] [INFO] [<module>] [23] : frame number: 23\n",
      "[2021-01-09 12:51:59,403] [INFO] [<module>] [23] : frame number: 24\n",
      "[2021-01-09 12:52:00,484] [INFO] [<module>] [23] : frame number: 25\n",
      "[2021-01-09 12:52:01,850] [INFO] [<module>] [23] : frame number: 26\n",
      "[2021-01-09 12:52:02,906] [INFO] [<module>] [23] : frame number: 27\n",
      "[2021-01-09 12:52:03,932] [INFO] [<module>] [23] : frame number: 28\n",
      "[2021-01-09 12:52:04,924] [INFO] [<module>] [23] : frame number: 29\n",
      "[2021-01-09 12:52:05,898] [INFO] [<module>] [23] : frame number: 30\n",
      "[2021-01-09 12:52:06,862] [INFO] [<module>] [23] : frame number: 31\n",
      "[2021-01-09 12:52:07,867] [INFO] [<module>] [23] : frame number: 32\n",
      "[2021-01-09 12:52:08,912] [INFO] [<module>] [23] : frame number: 33\n",
      "[2021-01-09 12:52:10,045] [INFO] [<module>] [23] : frame number: 34\n",
      "[2021-01-09 12:52:11,055] [INFO] [<module>] [23] : frame number: 35\n",
      "[2021-01-09 12:52:11,995] [INFO] [<module>] [23] : frame number: 36\n",
      "[2021-01-09 12:52:13,255] [INFO] [<module>] [23] : frame number: 37\n",
      "[2021-01-09 12:52:14,393] [INFO] [<module>] [23] : frame number: 38\n",
      "[2021-01-09 12:52:15,458] [INFO] [<module>] [23] : frame number: 39\n",
      "[2021-01-09 12:52:16,450] [INFO] [<module>] [23] : frame number: 40\n",
      "[2021-01-09 12:52:17,453] [INFO] [<module>] [23] : frame number: 41\n",
      "[2021-01-09 12:52:18,464] [INFO] [<module>] [23] : frame number: 42\n",
      "[2021-01-09 12:52:19,566] [INFO] [<module>] [23] : frame number: 43\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-25-55def1ac6a21>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[0;32mwhile\u001B[0m \u001B[0msuccess\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m     \u001B[0;31m# image = rotate_image(image, 90)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 19\u001B[0;31m     \u001B[0mparse_roi\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimage\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# build image ROI\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     20\u001B[0m     \u001B[0mimage\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcv2\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mimread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"faces_detected.jpg\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# possible BUG: read the same image twice if face not detected.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     21\u001B[0m     \u001B[0msuccess\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mimage\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mparse_RGB\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimage\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvidcap\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgreens\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreds\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mblues\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mframe_number\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-22-a1f0b662dfb6>\u001B[0m in \u001B[0;36mparse_roi\u001B[0;34m(img, method)\u001B[0m\n\u001B[1;32m     13\u001B[0m         \u001B[0mhogFaceDetector\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdlib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_frontal_face_detector\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m         \u001B[0mfaces\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhogFaceDetector\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgray\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m         \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfaces\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m             \u001B[0mlogging\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwarning\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"No face detected in image\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mrect\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mfaces\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "dataset_location = \"..\" + seperator + \"dataset\" + seperator + \"good_sync\" + seperator\n",
    "specific_dir = dir\n",
    "video_location = dataset_location + specific_dir + seperator + \"test.mp4\"\n",
    "logging.info(\"Working on video \" + video_location)\n",
    "vidcap = cv2.VideoCapture(video_location)\n",
    "success, image = vidcap.read()\n",
    "# image = cv2.rotate(image, cv2.cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "number_of_frames = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "x_value = np.arange((number_of_frames / np.round(fps)), step=(1 / 30))\n",
    "greens = np.zeros((1, int(number_of_frames)))  # instead of lists\n",
    "reds = np.zeros((1, int(number_of_frames)))\n",
    "blues = np.zeros((1, int(number_of_frames)))\n",
    "frame_number = 0\n",
    "logging.info(\"Parsing images ...\")\n",
    "while success:\n",
    "    # image = rotate_image(image, 90)\n",
    "    parse_roi(image)  # build image ROI\n",
    "    image = cv2.imread(\"faces_detected.jpg\") # possible BUG: read the same image twice if face not detected.\n",
    "    success, image = parse_RGB(image, vidcap, greens, reds, blues, frame_number)\n",
    "    frame_number += 1\n",
    "    logging.info(\"frame number: \" + str(frame_number))\n",
    "    # image = cv2.rotate(image, cv2.cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "    # cv2.imshow(\"Rotated (Correct)\", image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Butter filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%s\n"
    }
   },
   "outputs": [],
   "source": [
    "def apply_butter_filter(sig,fs):\n",
    "    \"\"\"\n",
    "    Filter signal using Butter filter and filtfilt. \n",
    "    :param sig: is the signal to be filtered\n",
    "    :return: the filtered signal\n",
    "    \"\"\"\n",
    "    logging.info(\"Applying butter filter...\")\n",
    "    # NOTE: 4 is the order,filternig valuesshould be between 0 to 1,\n",
    "    #Hence we multiply the freq. by 2 and divide by fs witch is the fps of the video.\n",
    "    bh,ah = signal.butter(4,4*2/fs,btype='highpass') # high pass at 4 hz Coefficients \n",
    "    bl,al = signal.butter(4,2/fs,btype='lowpass') # low pass at 1 hz  Coefficients\n",
    "    sig = signal.filtfilt(bh,ah,sig) # applying the filter coefficients on the signal.\n",
    "    sig = np.absolute(sig)\n",
    "    sig = signal.filtfilt(bl,al,sig)\n",
    "    return sig \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Plot after butter filter and the original signals ( A sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%s\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_result(greens, reds, blues, x_value, \"All 3 channels\") # original signals\n",
    "greens_buttered = apply_butter_filter(greens,fs=fps)\n",
    "blues_buttered = apply_butter_filter(blues,fs=fps)\n",
    "reds_buttered = apply_butter_filter(reds,fs=fps)\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.title(\"After filter\")\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(x_value.tolist(), greens_buttered.tolist()[0], color=\"green\")\n",
    "plt.grid(True)\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(x_value.tolist(), reds_buttered.tolist()[0], color=\"red\")\n",
    "plt.grid(True)\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(x_value.tolist(), blues_buttered.tolist()[0], color=\"blue\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "logging.info(\"Finished parsing the video.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%md\n"
    }
   },
   "outputs": [],
   "source": [
    "### pwelch testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,p = signal.welch(greens_buttered, fps, 'flattop', nperseg = 1024, scaling='spectrum')\n",
    "f = f.reshape(1,513).tolist()[0]\n",
    "a = np.sqrt(p)\n",
    "b = a.tolist()[0]\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.semilogy(f,b)\n",
    "plt.xlabel(\"freq. [Hz]\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}