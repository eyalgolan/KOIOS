{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic implementation using jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%s\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-02-28 09:31:08,734] [INFO] [<module>] [12] : Starting ...\n"
     ]
    }
   ],
   "source": [
    "import logging, numpy as np\n",
    "from scipy import signal\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import platform\n",
    "import dlib\n",
    "\n",
    "FORMAT = '[%(asctime)s] [%(levelname)s] [%(funcName)s] [%(lineno)d] : %(message)s'\n",
    "logging.basicConfig(format=FORMAT, level=logging.INFO)\n",
    "\n",
    "logging.info(\"Starting ...\")\n",
    "if platform.system() == \"Windows\":\n",
    "    seperator = \"\\\\\"\n",
    "else:\n",
    "    seperator = \"/\"\n",
    "\n",
    "dir = \"perry-all-2\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rotate image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%s\n"
    }
   },
   "outputs": [],
   "source": [
    "def rotate_image(mat, angle):\n",
    "    \"\"\"\n",
    "    Rotates an image (angle in degrees) and expands image to avoid cropping\n",
    "    \"\"\"\n",
    "\n",
    "    height, width = mat.shape[:2]  # image shape has 3 dimensions\n",
    "    image_center = (\n",
    "        width / 2,\n",
    "        height / 2)  # getRotationMatrix2D needs coordinates in reverse order (width, height) compared to shape\n",
    "\n",
    "    rotation_mat = cv2.getRotationMatrix2D(image_center, angle, 1.)\n",
    "\n",
    "    # rotation calculates the cos and sin, taking absolutes of those.\n",
    "    abs_cos = abs(rotation_mat[0, 0])\n",
    "    abs_sin = abs(rotation_mat[0, 1])\n",
    "\n",
    "    # find the new width and height bounds\n",
    "    bound_w = int(height * abs_sin + width * abs_cos)\n",
    "    bound_h = int(height * abs_cos + width * abs_sin)\n",
    "\n",
    "    # subtract old image center (bringing image back to origo) and adding the new image center coordinates\n",
    "    rotation_mat[0, 2] += bound_w / 2 - image_center[0]\n",
    "    rotation_mat[1, 2] += bound_h / 2 - image_center[1]\n",
    "\n",
    "    # rotate image with the new bounds and translated rotation matrix\n",
    "    rotated_mat = cv2.warpAffine(mat, rotation_mat, (bound_w, bound_h))\n",
    "    return rotated_mat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse roi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%s\n"
    }
   },
   "outputs": [],
   "source": [
    "cnn_face_detector = dlib.cnn_face_detection_model_v1(\"./models/mmod_human_face_detector.dat\")\n",
    "def parse_roi(img,method= \"dlib_cnn\"):\n",
    "    \"\"\"\n",
    "    Upon receiving an image, finds a face (if exists) and writes it as an image\n",
    "    :param image: the image to be parsed\n",
    "    \"\"\"\n",
    "    # The 1 in the second argument indicates that we should upsample the image\n",
    "    # 1 time.  This will make everything bigger and allow us to detect more\n",
    "    # faces.\n",
    "    if method == \"dlib_hog\":\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        hogFaceDetector = dlib.get_frontal_face_detector()\n",
    "        faces = hogFaceDetector(gray, 1)\n",
    "        if len(faces) ==0:\n",
    "            logging.warning(\"No face detected in image\")\n",
    "        for rect in faces:\n",
    "            x = rect.left()\n",
    "            y = rect.top()\n",
    "            w = rect.right() - x\n",
    "            h = rect.bottom() - y\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            roi_color = img[y:y + h, x:x + w]\n",
    "            cv2.imwrite(\"faces_detected.jpg\",roi_color)\n",
    "#             rgb = cv2.cvtColor(roi_color, cv2.COLOR_BGR2RGB)\n",
    "#             plt.imshow(rgb)\n",
    "#             plt.show()\n",
    "    else:\n",
    "        dets = cnn_face_detector(img, 1)\n",
    "        for d in dets:\n",
    "            flag_face_detected = True\n",
    "            crop = img[d.rect.top():d.rect.bottom(), d.rect.left():d.rect.right()]\n",
    "            cv2.imwrite(\"faces_detected.jpg\",crop)\n",
    "        if not flag_face_detected:\n",
    "                logging.warning(\"No face detected in image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%s\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_result(greens, reds, blues, x_value, title=\"\"):\n",
    "    logging.info(\"Plotting results ...\" + title)\n",
    "    x_value = x_value.reshape(1, x_value.shape[0])\n",
    "    r = reds.tolist()[0]\n",
    "    b = blues.tolist()[0]\n",
    "    g = greens.tolist()[0]\n",
    "    x = x_value.tolist()[0]\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(x, g, color=\"green\")\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(x, r, color=\"red\")\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(x, b, color=\"blue\")\n",
    "    plt.show()\n",
    "    logging.info(\"Showing result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%s\n"
    }
   },
   "outputs": [],
   "source": [
    "def parse_RGB(image, vidcap, greens, reds, blues, frame_number):\n",
    "    \"\"\"\n",
    "    Parses an image to its RGB channels\n",
    "    :param image: the image to be parsed\n",
    "    :param vidcap:\n",
    "    :param greens: array containing green channel values\n",
    "    :param blues: array containing blue channel values\n",
    "    :param reds: array containing red channel values\n",
    "    :param frame_number - is the number of the frame of the video.\n",
    "    :return: a flag indicating if there is a next image, and the next image\n",
    "    \"\"\"\n",
    "    blue, green, red = cv2.split(image)\n",
    "    greens[0, frame_number] = np.mean(green)\n",
    "    blues[0, frame_number] = np.mean(blue)\n",
    "    reds[0, frame_number] = np.mean(red)\n",
    "    success, image = vidcap.read()\n",
    "    return success, image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### read video frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%s\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-02-28 09:31:09,072] [INFO] [<module>] [5] : Working on video ..\\dataset\\good_sync\\perry-all-2\\ptest.mp4\n",
      "d:\\studies\\bar_ilan\\year 3\\final_project\\koios\\venv\\lib\\site-packages\\ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "arange: cannot compute length",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-6-82abe38500aa>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0mfps\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvidcap\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcv2\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mCAP_PROP_FPS\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0mnumber_of_frames\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvidcap\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcv2\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mCAP_PROP_FRAME_COUNT\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m \u001B[0mx_value\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnumber_of_frames\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mround\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfps\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstep\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0;36m30\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m \u001B[0mgreens\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzeros\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnumber_of_frames\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# instead of lists\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0mreds\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzeros\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnumber_of_frames\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: arange: cannot compute length"
     ]
    }
   ],
   "source": [
    "dataset_location = \"..\" + seperator + \"dataset\" + seperator + \"good_sync\" + seperator\n",
    "specific_dir = dir\n",
    "video_location = dataset_location + specific_dir + seperator + \"ptest.mp4\"\n",
    "logging.info(\"Working on video \" + video_location)\n",
    "vidcap = cv2.VideoCapture(video_location)\n",
    "success, image = vidcap.read()\n",
    "# image = cv2.rotate(image, cv2.cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "number_of_frames = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "x_value = np.arange((number_of_frames / np.round(fps)), step=(1 / 30))\n",
    "greens = np.zeros((1, int(number_of_frames)))  # instead of lists\n",
    "reds = np.zeros((1, int(number_of_frames)))\n",
    "blues = np.zeros((1, int(number_of_frames)))\n",
    "frame_number = 0\n",
    "logging.info(\"Parsing images ...\")\n",
    "while success:\n",
    "    # image = rotate_image(image, 90)\n",
    "    parse_roi(image)  # build image ROI\n",
    "    image = cv2.imread(\"faces_detected.jpg\") # possible BUG: read the same image twice if face not detected.\n",
    "    success, image = parse_RGB(image, vidcap, greens, reds, blues, frame_number)\n",
    "    frame_number += 1\n",
    "    logging.info(\"frame number: \" + str(frame_number))\n",
    "    # image = cv2.rotate(image, cv2.cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "    # cv2.imshow(\"Rotated (Correct)\", image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Butter filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%s\n"
    }
   },
   "outputs": [],
   "source": [
    "def apply_butter_filter(sig,fs):\n",
    "    \"\"\"\n",
    "    Filter signal using Butter filter and filtfilt. \n",
    "    :param sig: is the signal to be filtered\n",
    "    :return: the filtered signal\n",
    "    \"\"\"\n",
    "    logging.info(\"Applying butter filter...\")\n",
    "    # NOTE: 4 is the order,filternig valuesshould be between 0 to 1,\n",
    "    #Hence we multiply the freq. by 2 and divide by fs witch is the fps of the video.\n",
    "    bh,ah = signal.butter(4,4*2/fs,btype='highpass') # high pass at 4 hz Coefficients \n",
    "    bl,al = signal.butter(4,2/fs,btype='lowpass') # low pass at 1 hz  Coefficients\n",
    "    sig = signal.filtfilt(bh,ah,sig) # applying the filter coefficients on the signal.\n",
    "    sig = np.absolute(sig)\n",
    "    sig = signal.filtfilt(bl,al,sig)\n",
    "    return sig \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Plot after butter filter and the original signals ( A sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%s\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_result(greens, reds, blues, x_value, \"All 3 channels\") # original signals\n",
    "greens_buttered = apply_butter_filter(greens,fs=fps)\n",
    "blues_buttered = apply_butter_filter(blues,fs=fps)\n",
    "reds_buttered = apply_butter_filter(reds,fs=fps)\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.title(\"After filter\")\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(x_value.tolist(), greens_buttered.tolist()[0], color=\"green\")\n",
    "plt.grid(True)\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(x_value.tolist(), reds_buttered.tolist()[0], color=\"red\")\n",
    "plt.grid(True)\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(x_value.tolist(), blues_buttered.tolist()[0], color=\"blue\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "logging.info(\"Finished parsing the video.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%md\n"
    }
   },
   "outputs": [],
   "source": [
    "### pwelch testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,p = signal.welch(greens_buttered, fps, 'flattop', nperseg = 1024, scaling='spectrum')\n",
    "f = f.reshape(1,513).tolist()[0]\n",
    "a = np.sqrt(p)\n",
    "b = a.tolist()[0]\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.semilogy(f,b)\n",
    "plt.xlabel(\"freq. [Hz]\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}