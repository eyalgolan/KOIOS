{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from moviepy.editor import *\n",
    "#your_video = \"../dataset/good_sync/perry-all-2/P104_2020-12-06_121257717.mp4\"\n",
    "video_sources = [\"first_video.mp4\", \"second_video.mp4\",\"third_video.mp4\",\n",
    "                 \"fourth_video.mp4\", \"sixth_video.mp4\",\n",
    "                 \"seventh_video.mp4\", \"eight_video.mp4\", \"ninth_video.mp4\"]\n",
    "rotated_video_sources = [\"rotated_\" + source for source in video_sources]\n",
    "rotated_video_sources = ['/Users/perrytubul/Desktop/computerscience/year3/Kiois/OurOwnRepo/DATASET_2/subject1/vid.avi']\n",
    "#for video_source, rotated_source in zip(video_sources, rotated_video_sources):\n",
    "#    clip = VideoFileClip(video_source)\n",
    "#    clip = clip.rotate(90)\n",
    "#    clip.write_videofile(rotated_source)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic engine implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import face_recognition, PIL.Image, PIL.ImageDraw,math\n",
    "import numpy as np\n",
    "import logging\n",
    "import cv2\n",
    "import platform\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import scipy.signal as sig\n",
    "import pywt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.covariance import MinCovDet as MCD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "FORMAT = '[%(asctime)s] [%(levelname)s] [%(funcName)s] [%(lineno)d] : %(message)s'\n",
    "logging.basicConfig(format=FORMAT, level=logging.INFO)\n",
    "\n",
    "logging.info(\"Starting ...\")\n",
    "if platform.system() == \"Windows\":\n",
    "    seperator = \"\\\\\"\n",
    "else:\n",
    "    seperator = \"/\"\n",
    "\n",
    "dir = \"perry-all-2\"\n",
    "# should be a parameter of the engine\n",
    "dataset_location = \"..\" + seperator + \"dataset\" + seperator + \"good_sync\" + seperator\n",
    "specific_dir = dir\n",
    "#video_location = dataset_location + specific_dir + seperator + \"lab1.mp4\"\n",
    "#video_location = \"rotated.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## running evm pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#%run ./evm_preprocessing.ipynb\n",
    "# video_location = dataset_location + specific_dir + seperator + \"out.avi\"\n",
    "#video_location=\"out2.avi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detecting the face landmarks and parsing the ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def parse_roi(frame):\n",
    "    # image = face_recognition.load_image_file(frame) # read image.\n",
    "    face_locations = face_recognition.face_locations(frame,model = 'hog') # detects all the faces in image\n",
    "    face_landmarks_list = face_recognition.face_landmarks(image)\n",
    "    \n",
    "    # iterate through all the faces.\n",
    "    for face_location in face_locations:\n",
    "        img = PIL.Image.fromarray(frame)\n",
    "        top,right,bottom,left = face_location # extract all face square points.\n",
    "        diff = math.floor((top - bottom) * 0.15) # 20 percent of the face len (toadd eyebrow top point).\n",
    "        \n",
    "        # finding the forehead\n",
    "        try:\n",
    "            right_eyebrow_landmarks = np.asarray(face_landmarks_list[0]['right_eyebrow']) # right eyebrow points.\n",
    "        except:\n",
    "            return None\n",
    "        right_eyebrow_landmarks.sort(axis=0)\n",
    "        rightest_point = right_eyebrow_landmarks[-1] # The most right point of the ROI(according to x).\n",
    "        top_right_eyebrow = right_eyebrow_landmarks.min(axis = 0)[1]\n",
    "        try:\n",
    "            left_eyebrow_landmarks = np.asarray(face_landmarks_list[0]['left_eyebrow'])\n",
    "        except:\n",
    "            return None\n",
    "        left_eyebrow_landmarks.sort(axis=0)\n",
    "        leftest_point = left_eyebrow_landmarks[0] # the most left point of ROI.(according to x)\n",
    "        top_left_eyebrow = left_eyebrow_landmarks.min(axis = 0)[1]\n",
    "        bottom = min(top_right_eyebrow,top_left_eyebrow).item(0) # bottom point of the forehead.\n",
    "        bottom = bottom - (0.05 * bottom) # improve bottom location by 2 percent.\n",
    "        forehead = img.crop((leftest_point[0], leftest_point[1]+diff, rightest_point[0],bottom+10)) # adding diff to top to make the forehead bigger.\n",
    "\n",
    "        # finding the second ROI:\n",
    "        try:\n",
    "            upper_mouth = np.asarray(face_landmarks_list[0]['top_lip']) # top_lip landmarks\n",
    "        except:\n",
    "            return None\n",
    "        upper_mouth_min = upper_mouth.min(axis = 0)[1] # The  top - lip upper point.\n",
    "        try:\n",
    "            upper_nose = np.asarray(face_landmarks_list[0]['nose_bridge'])\n",
    "        except:\n",
    "            return None\n",
    "        upper_nose_min = upper_nose.min(axis = 0)[1]  # noise bridge upper point.\n",
    "        upper_nose_min += upper_mouth_min * 0.1 # improving the noise bridge upper point.\n",
    "        nose_to_upper_lip = img.crop((leftest_point[0], upper_nose_min, rightest_point[0], upper_mouth_min))\n",
    "\n",
    "        return forehead, nose_to_upper_lip\n",
    "    return None # in case of which no face was detected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for bad frames\n",
    "##### R > 95 and G > 40 and B > 20 and R > G and R > B\n",
    "##### Based on https://arxiv.org/ftp/arxiv/papers/1708/1708.02694.pdf page 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%s\n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "red_min_val = 95\n",
    "green_min_val = 40\n",
    "blue_min_val = 20\n",
    "red_green_max_diff = 15\n",
    "def good_frame(blue, green, red):\n",
    "    if red <= red_min_val:\n",
    "        logging.warning(\"bad frame detected, reason: red > red_min_val\")\n",
    "        return False\n",
    "    if green <= green_min_val:\n",
    "        logging.warning(\"bad frame detected, reason: green > green_min_val\")\n",
    "        return False\n",
    "    if blue <= blue_min_val:\n",
    "        logging.warning(\"bad frame detected, reason: blue > blue_min_val\")\n",
    "        return False\n",
    "    if red <= green:\n",
    "        logging.warning(\"bad frame detected, reason: red > green\")\n",
    "        return False\n",
    "    if red <= blue:\n",
    "        logging.warning(\"bad frame detected, reason: red > blue\")\n",
    "        return False\n",
    "    if abs(red - green) <= red_green_max_diff:\n",
    "        logging.warning(\"bad frame detected, reason: abs(red - green) > red_green_max_diff\")\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_new_frame(vidcap):\n",
    "    success, next_image = vidcap.read()\n",
    "    return success, next_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting RGB arrays results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_result(greens, reds, blues, x_value, title=\"\"):\n",
    "    logging.info(\"Plotting results ...\" + title)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(x_value, greens, color=\"green\")\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(x_value, reds, color=\"red\")\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(x_value, blues, color=\"blue\")\n",
    "    plt.show()\n",
    "    logging.info(\"Showing result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def filter_channel(channel,fs):\n",
    "    \"\"\"\n",
    "    This method apply filter on a channel between 0.75HZ to 4HZ.\n",
    "    :param channel: Is a signal to apply the filter to.\n",
    "    :param fs: Is the sampling rate of channel.\n",
    "    :return: The filtered channel.\n",
    "    \"\"\"\n",
    "    bh, ah = sig.butter(4, 0.75 / (fs / 2), 'highpass')\n",
    "    bl, al = sig.butter(4, 4 / (fs / 2), 'lowpass')\n",
    "    try:\n",
    "        channel = sig.filtfilt(bh, ah, channel) # applying the filter coefficient on the sig\n",
    "    except:\n",
    "        return None\n",
    "    #channel = np.absolute(channel)\n",
    "    channel_after_filter = sig.filtfilt(bl, al, channel) # applying the filter coefficient on the sig\n",
    "    return channel_after_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting RGB values from a frame and adding them to arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def parse_luminace(red, green, blue):\n",
    "    luminance_level = 0.2126 * red + 0.7152 * green + 0.0722 * blue\n",
    "    return luminance_level\n",
    "def parse_RGB(rois, color_sig):\n",
    "    \"\"\"\n",
    "    Parses an image to its RGB channels\n",
    "    :param image: the image to be parsed\n",
    "    :param vidcap:\n",
    "    :param greens: array containing green channel values\n",
    "    :param blues: array containing blue channel values\n",
    "    :param reds: array containing red channel values\n",
    "    :param frame_number - is the number of the frame of the video.\n",
    "    :return: a flag indicating if there is a next image, and the next image\n",
    "    \"\"\"\n",
    "    \n",
    "    for i,r in enumerate(rois):\n",
    "        try:\n",
    "            new_red = r.getchannel(0)\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "        new_green = r.getchannel(1)\n",
    "        new_blue = r.getchannel(2)\n",
    "        b_mean,g_mean,r_mean = np.mean(new_blue),np.mean(new_green),np.mean(new_red)\n",
    "        luminance_level = parse_luminace(r_mean, g_mean, b_mean)\n",
    "        if good_frame(b_mean,g_mean,r_mean):\n",
    "            #color_channels = r.reshape(-1, r.shape[-1])\n",
    "            #avg_color = color_channels.mean(axis=0)\n",
    "            color_sig[i].append(g_mean)\n",
    "    return True, color_sig, luminance_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def log_video_details(video_source):\n",
    "    logging.info(\"\\nInformation on video:\\t\\t\\t\\t\\t\\t\" + str(video_source) +\n",
    "                 \"\\nFPS:\\t\\t\\t\\t\\t\\t\" + str(fps) + \n",
    "                 \"\\nRound FPS:\\t\\t\\t\\t\\t\\t\" + str(round_fps) + \n",
    "                 \"\\nNumber of frames:\\t\\t\\t\\t\" + str(number_of_frames) + \n",
    "                 \"\\nNumber of bad frames:\\t\\t\\t\\t\" + str(bad_frames) + \n",
    "                 \"\\nMax luminanace:\\t\\t\\t\\t\\t\" + str(max_luminance) + \n",
    "                 \"\\nMin luminanace:\\t\\t\\t\\t\\t\" + str(min_luminance) +\n",
    "                 \"\\nMax diff of luminanace between adjacent frames:\\t\" + str(max_diff_luminance_adjacent) +\n",
    "                 \"\\nAvg luminanace:\\t\\t\\t\\t\\t\" + str(avg_luminance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def indexes(y, thres=0.3, min_dist=1, thres_abs=False):\n",
    "    if isinstance(y, np.ndarray) and np.issubdtype(y.dtype, np.unsignedinteger):\n",
    "        raise ValueError(\"y must be signed\")\n",
    "\n",
    "    if not thres_abs:\n",
    "        thres = thres * (np.max(y) - np.min(y)) + np.min(y)\n",
    "\n",
    "    min_dist = int(min_dist)\n",
    "\n",
    "    # compute first order difference\n",
    "    dy = np.diff(y)\n",
    "\n",
    "    # propagate left and right values successively to fill all plateau pixels (0-value)\n",
    "    zeros, = np.where(dy == 0)\n",
    "\n",
    "    # check if the signal is totally flat\n",
    "    if len(zeros) == len(y) - 1:\n",
    "        return np.array([])\n",
    "\n",
    "    if len(zeros):\n",
    "        # compute first order difference of zero indexes\n",
    "        zeros_diff = np.diff(zeros)\n",
    "        # check when zeros are not chained together\n",
    "        zeros_diff_not_one, = np.add(np.where(zeros_diff != 1), 1)\n",
    "        # make an array of the chained zero indexes\n",
    "        zero_plateaus = np.split(zeros, zeros_diff_not_one)\n",
    "\n",
    "        # fix if leftmost value in dy is zero\n",
    "        if zero_plateaus[0][0] == 0:\n",
    "            dy[zero_plateaus[0]] = dy[zero_plateaus[0][-1] + 1]\n",
    "            zero_plateaus.pop(0)\n",
    "\n",
    "        # fix if rightmost value of dy is zero\n",
    "        if len(zero_plateaus) and zero_plateaus[-1][-1] == len(dy) - 1:\n",
    "            dy[zero_plateaus[-1]] = dy[zero_plateaus[-1][0] - 1]\n",
    "            zero_plateaus.pop(-1)\n",
    "\n",
    "        # for each chain of zero indexes\n",
    "        for plateau in zero_plateaus:\n",
    "            median = np.median(plateau)\n",
    "            # set leftmost values to leftmost non zero values\n",
    "            dy[plateau[plateau < median]] = dy[plateau[0] - 1]\n",
    "            # set rightmost and middle values to rightmost non zero values\n",
    "            dy[plateau[plateau >= median]] = dy[plateau[-1] + 1]\n",
    "\n",
    "    # find the peaks by using the first order difference\n",
    "    peaks = np.where(\n",
    "        (np.hstack([dy, 0.0]) < 0.0)\n",
    "        & (np.hstack([0.0, dy]) > 0.0)\n",
    "        & (np.greater(y, thres))\n",
    "    )[0]\n",
    "\n",
    "    # handle multiple peaks, respecting the minimum distance\n",
    "    if peaks.size > 1 and min_dist > 1:\n",
    "        highest = peaks[np.argsort(y[peaks])][::-1]\n",
    "        rem = np.ones(y.size, dtype=bool)\n",
    "        rem[peaks] = False\n",
    "\n",
    "        for peak in highest:\n",
    "            if not rem[peak]:\n",
    "                sl = slice(max(0, peak - min_dist), peak + min_dist + 1)\n",
    "                rem[sl] = True\n",
    "                rem[peak] = False\n",
    "\n",
    "        peaks = np.arange(y.size)[~rem]\n",
    "\n",
    "    return peaks\n",
    "\n",
    "def print_results(window_sig, window, xlabel, ylabel, change_range, title):\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle(title)\n",
    "    ax = fig.subplots()\n",
    "    while len(window_sig) > len(window):\n",
    "        window_sig = window_sig[:-1]\n",
    "    ax.plot(window_sig,window,color ='green')\n",
    "    if change_range:\n",
    "        plt.xlim([0, 5])\n",
    "\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xscale('linear')\n",
    "    ax.spines['bottom'].set_color('red')\n",
    "    ax.spines['left'].set_color('red')\n",
    "    ax.xaxis.label.set_color('red')\n",
    "    ax.yaxis.label.set_color('red')\n",
    "    ax.tick_params(axis='x', colors='red')\n",
    "    ax.tick_params(axis='y', colors='red')\n",
    "    plt.show()\n",
    "\n",
    "import scipy\n",
    "def find_peaks_with_scipy(window_sig, window, round_fps, hr):\n",
    "    peaks, _ = scipy.signal.find_peaks(window, distance=round_fps)\n",
    "    plt.plot(window_sig,window,'-go',markerfacecolor='red',markevery=peaks)\n",
    "    plt.title(\"Peaks with scipy\")\n",
    "    plt.show()\n",
    "    logging.info(\"Peaks vector with scipy: \" + str(peaks) + \" num of peaks: \" + str(len(peaks)))\n",
    "\n",
    "def print_peaks(window_sig, window):\n",
    "    while len(window_sig) > len(window):\n",
    "        window_sig = window_sig[:-1]\n",
    "        \n",
    "    peaks = indexes(window,min_dist=20)\n",
    "    plt.plot(window_sig,window,'-go',markerfacecolor='red',markevery=peaks)\n",
    "    plt.title(\"Peaks with implemented function\")\n",
    "    plt.show()\n",
    "    logging.info(\"Peaks vector by implemented function: \" + str(window_sig[peaks]) + \" num of peaks: \" + str(len(window_sig[peaks])))\n",
    "    #print(window_sig)\n",
    "    #test_peaks = find_peaks(window_sig)\n",
    "    #plt.plot(window_sig,window,'-go',markerfacecolor='red',markevery=test_peaks)\n",
    "    #np.savetxt(\"time_of_peaks.csv\", x[peaks], delimiter=\",\")\n",
    "    \n",
    "def find_hr_in_window(green, window_start, round_fps, window_id, window_size):\n",
    "    \n",
    "    round_fps = int(round_fps)\n",
    "    if window_start + round_fps * window_size > len(green):\n",
    "        window = green[window_start : ]\n",
    "    else:\n",
    "        window = green[window_start : window_start + round_fps * window_size]\n",
    "    window_sig = np.arange(window.size/round_fps,step= (1/round_fps))\n",
    "    \n",
    "    print_results(window_sig, window, 'X-axis', 'Y-axis', False, \"Green signal\")\n",
    "\n",
    "    window = window - np.mean(window)\n",
    "    window = window / np.std(window)\n",
    "    print_results(window_sig, window, 'X-axis', 'Y-axis', False, \"Green signal normalized\")\n",
    "    \n",
    "    g = filter_channel(window,round_fps)\n",
    "    if g is None:\n",
    "        return\n",
    "    from skimage.restoration import denoise_wavelet\n",
    "    #g = denoise_wavelet(g,wavelet='sym2',wavelet_levels = 4)\n",
    "    print_peaks(window_sig, g)\n",
    "    print_results(window_sig, window, 'X-axis', 'Y-axis', False, \"\")\n",
    "\n",
    "    f, Pxx_den = sig.periodogram(g, round_fps)\n",
    "    \n",
    "    print_results(f, Pxx_den, 'Frequency [Hz]', 'PSD [V**2/Hz]', True, \"PSD by Frequency\")\n",
    "    # plt.semilogy(f, Pxx_den)\n",
    "    # plt.ylim([1e-7, 1e2])\n",
    "    # plt.xlabel('frequency [Hz]')\n",
    "    # plt.ylabel('PSD [V**2/Hz]')\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    max_val = Pxx_den.argmax()\n",
    "    logging.info(\"Window \" + str(window_id) +\n",
    "                 \":\\nHighest freq:\" + str(f[max_val]) + \"\\nHeart rate: \" + str(f[max_val]*60))\n",
    "    \n",
    "    find_peaks_with_scipy(window_sig, g,round_fps, f[max_val]*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pywt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.covariance import MinCovDet as MCD\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skimage.restoration import denoise_wavelet\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy,itertools\n",
    "from skimage.restoration import denoise_wavelet\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "def eigenvalue_decomposition(mat):\n",
    "        \"\"\" return (diagonal,orthogonal matrix)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # eigen_vals, eigen_vectors\n",
    "        eigen_vals, eigen_vectors = np.linalg.eigh(mat)\n",
    "        d = np.diag(eigen_vals)\n",
    "        return d,eigen_vectors\n",
    "\n",
    "    \n",
    "def multivariate_video_signal(signal):\n",
    "    \"\"\"\n",
    "    Algorithm 1\n",
    "    :param signal: is a 2D signal. One dimension the signal from the forehead, and second is from above the upper lip\n",
    "    :return: multivariate PPG \n",
    "    \"\"\"    \n",
    "    sig1 = signal[0,:].T\n",
    "    sig2 = signal[1,:].T\n",
    "    n = len(sig1)\n",
    "    wavelet = pywt.Wavelet('sym2') \n",
    "    c = pywt.wavedec(np.column_stack((sig1,sig2)),wavelet,level = 4,axis = 0)\n",
    "    mcd = MCD(random_state = 0).fit(c[-1]).covariance_\n",
    "    V,D,VT = np.linalg.svd(mcd)\n",
    "\n",
    "    lst = []\n",
    "    pca = PCA(1)\n",
    "    comp = StandardScaler().fit_transform(c[0]) # normalize\n",
    "    c[0] = comp\n",
    "    comp = pca.fit_transform(comp)\n",
    "    comp = np.column_stack((comp,comp))\n",
    "    lst.append(comp)\n",
    "    # Basis change \n",
    "    for i in range (1,len(c)):\n",
    "        x_i = np.dot(c[i],V)\n",
    "        gama_0 = np.sqrt(2*np.log(n)*D[0])\n",
    "        xi_0 = pywt.threshold(x_i[:,0],value= gama_0,mode='soft')\n",
    "        gama_1 = np.sqrt(2*np.log(n)*D[1])\n",
    "        xi_1 = pywt.threshold(x_i[:,1],value= gama_1,mode='soft')\n",
    "        xi = np.column_stack((xi_0,xi_1))\n",
    "        xi = xi@VT\n",
    "        lst.append(xi)\n",
    "    return pywt.waverec(lst,wavelet,axis = 0)\n",
    "\n",
    "def mvd(color_sig):\n",
    "    ppg_hat = multivariate_video_signal(color_sig)\n",
    "    return ppg_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def detect_hr(video_source):\n",
    "    logging.info(\"\\n=======================\\n\" + video_source + \"\\n=======================\\n\")\n",
    "    color_sig_array = np.asarray(color_sig)\n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(color_sig_array[0],\"green\")\n",
    "    sig_mvd = mvd(color_sig_array) # n X 2\n",
    "    ppg_hat_zero = filter_channel(sig_mvd[:,0].T,round_fps)\n",
    "    ppg_hat_one = filter_channel(sig_mvd[:,1].T,round_fps)\n",
    "    ppg_hat = np.vstack((ppg_hat_zero,ppg_hat_one))\n",
    "    plt.subplot(2,1,2)\n",
    "    green = ppg_hat[0]\n",
    "    plt.plot(green,\"red\")\n",
    "    #blue = color_sig_array[0][:,2]\n",
    "\n",
    "    window_start = 0\n",
    "    window_size = 30\n",
    "    window_id = 0\n",
    "    limit = good_frame_number - int(round_fps) * window_size\n",
    "    while window_start < limit :\n",
    "        find_hr_in_window(green, window_start, round_fps, window_id, window_size)\n",
    "        window_start += int(round_fps) * window_size\n",
    "        window_id += 1\n",
    "    if window_start < good_frame_number:\n",
    "        find_hr_in_window(green, window_start, round_fps, window_id, good_frame_number - window_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Main loop - going over all the frames of the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Parsing video:\n",
    "for video_location in rotated_video_sources:\n",
    "    color_sig = [[],[]]\n",
    "    heart_rates = []\n",
    "    good_frame_number = 0\n",
    "#     total_frame_number = 180\n",
    "    total_frame_number = 0\n",
    "    logging.info(\"Working on video \" + video_location)\n",
    "    vidcap = cv2.VideoCapture(video_location)\n",
    "    success, image = vidcap.read()\n",
    "    fps = vidcap.get(cv2.CAP_PROP_FPS) # fs == sampling rate\n",
    "    round_fps = np.round(fps)\n",
    "    number_of_frames = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    logging.info(\"Parsing images ...\")\n",
    "    skipped_frames = 0\n",
    "    bad_frames = 0\n",
    "    max_luminance = 0\n",
    "    min_luminance = 200\n",
    "    avg_luminance = 0\n",
    "    perv_luminance = None\n",
    "    max_diff_luminance_adjacent = 0\n",
    "\n",
    "    while success:\n",
    "#         if skipped_frames < 180:\n",
    "#             skipped_frames += 1\n",
    "#             success, image = get_new_frame(vidcap)\n",
    "#             continue\n",
    "        if total_frame_number % 100 == 0:\n",
    "            logging.info(\"Parsing frame \" + str(total_frame_number) + \"/\" + str(number_of_frames))\n",
    "        rois = parse_roi(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))  # build image ROI (rois is a tuple contains two regions)\n",
    "        if rois is not None :\n",
    "            #roi = np.vstack((rois[0], rois[1]))\n",
    "            try:\n",
    "                is_good_frame,color_sig, luminance_level = parse_RGB(rois, color_sig)\n",
    "                if perv_luminance is not None and luminance_level - perv_luminance > max_diff_luminance_adjacent:\n",
    "                    max_diff_luminance_adjacent = luminance_level - perv_luminance\n",
    "                if luminance_level > max_luminance:\n",
    "                    max_luminance = luminance_level\n",
    "                if luminance_level < min_luminance:\n",
    "                    min_luminance = luminance_level\n",
    "                avg_luminance += luminance_level\n",
    "                perv_luminance = luminance_level\n",
    "            except Exception as e:\n",
    "                logging.error(\"failed to get output from parse_RGB!\\nError:\" + str(e))\n",
    "                is_good_frame = False\n",
    "                bad_frames += 1\n",
    "            if is_good_frame:\n",
    "                good_frame_number += 1\n",
    "                #logging.info(\"luminance level: \" + str(luminance_level))\n",
    "        if rois is None:\n",
    "            bad_frames += 1\n",
    "        total_frame_number += 1\n",
    "        success, image = get_new_frame(vidcap)\n",
    "    avg_luminance /= total_frame_number\n",
    "    log_video_details(video_location)\n",
    "    detect_hr(video_location)\n",
    "#     except:\n",
    "#        logging.warning(\"Issue in detecting hr in video\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "detect_hr(\"First Video\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}