{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced engine implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup - Logger, videos location, OS detection, MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN #for detecting ROI of a moving subject\n",
    "import torch\n",
    "import face_recognition, PIL.Image, PIL.ImageDraw,math\n",
    "import logging\n",
    "import cv2\n",
    "import platform\n",
    "import scipy.signal as sig\n",
    "import os\n",
    "import pywt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.covariance import MinCovDet as MCD\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "\n",
    "FORMAT = '[%(asctime)s] [%(levelname)s] [%(funcName)s] [%(lineno)d] : %(message)s'\n",
    "logging.basicConfig(format=FORMAT, level=logging.INFO)\n",
    "\n",
    "with open(\"input_path.txt\") as input_file:\n",
    "    input_location = input_file.read()\n",
    "pattern = \".avi\"\n",
    "video_sources = []\n",
    "\n",
    "for path, subdirs, files in os.walk(input_location):\n",
    "    for name in files:\n",
    "        if name.endswith(pattern):\n",
    "            video_sources.append(os.path.join(path, name))\n",
    "\n",
    "print(video_sources)\n",
    "logging.info(\"Starting ...\")\n",
    "if platform.system() == \"Windows\":\n",
    "    seperator = \"\\\\\"\n",
    "else:\n",
    "    seperator = \"/\"\n",
    "\n",
    "\n",
    "# since MTCNN is a collection of neural nets and other code, the device must be passed\n",
    "# in the following way to enable copying of objects when needed internally\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "mtcnn = MTCNN(keep_all=True, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running evm pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#%run ./evm_preprocessing.ipynb\n",
    "# video_location = dataset_location + specific_dir + seperator + \"out.avi\"\n",
    "#video_location=\"out2.avi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detects face landmarks and parsing the ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def detect_face(frame):\n",
    "    \"\"\"\n",
    "    Detect face in a frame\n",
    "    :param frame: A video frame\n",
    "    :return: the location of the face in the picture\n",
    "    \"\"\"\n",
    "    face_locations_handle_motion = mtcnn.detect(frame) # using pre-trained model to find faces\n",
    "    face_location = list() \n",
    "    face_location.append(float(face_locations_handle_motion[0][0][1]))\n",
    "    face_location.append(float(face_locations_handle_motion[0][0][2]))\n",
    "    face_location.append(float(face_locations_handle_motion[0][0][3]))\n",
    "    face_location.append(float(face_locations_handle_motion[0][0][0]))\n",
    "    face_locations = list()\n",
    "    face_locations.append(face_location)\n",
    "    return face_locations\n",
    "\n",
    "def parse_roi(frame):\n",
    "    \"\"\"\n",
    "    Find a face and its region of interests.  \n",
    "    @param frame: A video frame\n",
    "    @return: None in case of which no face was detected. A tuple in a form of (forehead, nose_to_upper_lip) \n",
    "    forehead is ndarray that represents the subject's forehead, nose_to_upper_lip is ndarray that represents the region\n",
    "    between the upper lip and the nose of the subject. \n",
    "    \"\"\"\n",
    "    # image = face_recognition.load_image_file(frame) # read image.\n",
    "    face_locations = face_recognition.face_locations(frame,model = 'hog') # detects all the faces in image\n",
    "    face_landmarks_list = face_recognition.face_landmarks(image)\n",
    "    \n",
    "    try:\n",
    "        face_locations = detect_face(frame)\n",
    "    except Exception as e:\n",
    "        logging.error(\"Failure in face detection, error: \" + str(e))\n",
    "        \n",
    "    # iterate through all the faces.\n",
    "    for face_location in face_locations:\n",
    "        img = PIL.Image.fromarray(frame)\n",
    "        top,right,bottom,left = face_location # extract all face square points.\n",
    "        diff = math.floor((top - bottom) * 0.15) # 20 percent of the face len (toadd eyebrow top point).\n",
    "        \n",
    "        # finding the forehead\n",
    "        try:\n",
    "            right_eyebrow_landmarks = np.asarray(face_landmarks_list[0]['right_eyebrow']) # right eyebrow points.\n",
    "        except Exception as e:\n",
    "            logging.warning(\"No forehead found, \" + str(e))\n",
    "            return None\n",
    "        right_eyebrow_landmarks.sort(axis=0)\n",
    "        rightest_point = right_eyebrow_landmarks[-1] # The most right point of the ROI(according to x).\n",
    "        top_right_eyebrow = right_eyebrow_landmarks.min(axis = 0)[1]\n",
    "        try:\n",
    "            left_eyebrow_landmarks = np.asarray(face_landmarks_list[0]['left_eyebrow'])\n",
    "        except Exception as e:\n",
    "            logging.warning(\"No left eyebrow found, \" + str(e))\n",
    "            return None\n",
    "        left_eyebrow_landmarks.sort(axis=0)\n",
    "        leftest_point = left_eyebrow_landmarks[0] # the most left point of ROI.(according to x)\n",
    "        top_left_eyebrow = left_eyebrow_landmarks.min(axis = 0)[1]\n",
    "        bottom = min(top_right_eyebrow,top_left_eyebrow).item(0) # bottom point of the forehead.\n",
    "        bottom = bottom - (0.05 * bottom) # improve bottom location by 2 percent.\n",
    "        forehead = img.crop((leftest_point[0], leftest_point[1]+diff, rightest_point[0],bottom+10)) # adding diff to top to make the forehead bigger.\n",
    "\n",
    "        # finding the second ROI:\n",
    "        try:\n",
    "            upper_mouth = np.asarray(face_landmarks_list[0]['top_lip']) # top_lip landmarks\n",
    "        except Exception as e:\n",
    "            logging.warning(\"No upper mouth found, \" + str(e))\n",
    "            return None\n",
    "        upper_mouth_min = upper_mouth.min(axis = 0)[1] # The  top - lip upper point.\n",
    "        try:\n",
    "            upper_nose = np.asarray(face_landmarks_list[0]['nose_bridge'])\n",
    "        except Exception as e:\n",
    "            logging.warning(\"No upper nose found, \" + str(e))\n",
    "            return None\n",
    "        upper_nose_min = upper_nose.min(axis = 0)[1]  # noise bridge upper point.\n",
    "        upper_nose_min += upper_mouth_min * 0.1 # improving the noise bridge upper point.\n",
    "        nose_to_upper_lip = img.crop((leftest_point[0], upper_nose_min, rightest_point[0], upper_mouth_min))\n",
    "\n",
    "        return forehead, nose_to_upper_lip\n",
    "    return None # in case of which no face was detected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for bad frames\n",
    "##### Criteria is:  R > 95 and G > 40 and B > 20 and R > G and R > B\n",
    "##### Based on https://arxiv.org/ftp/arxiv/papers/1708/1708.02694.pdf page 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%s\n"
    }
   },
   "outputs": [],
   "source": [
    "red_min_val = 95\n",
    "green_min_val = 40\n",
    "blue_min_val = 20\n",
    "red_green_max_diff = 15\n",
    "def good_frame(blue, green, red):\n",
    "    \"\"\"\n",
    "        Checks the light conditions \n",
    "        @param: blue Is the mean of all the blue pixels in one frame\n",
    "        @param: green Is the mean  of all the green pixels in one frame\n",
    "        @param: red Is the mean of all the red pixels in one frame\n",
    "        return False 0 meaning the lightning are bad, True when the lightning condition are OK\n",
    "    \"\"\"\n",
    "    if red <= red_min_val:\n",
    "        logging.warning(\"bad frame detected, reason: red > red_min_val\")\n",
    "        return False\n",
    "    if green <= green_min_val:\n",
    "        logging.warning(\"bad frame detected, reason: green > green_min_val\")\n",
    "        return False\n",
    "    if blue <= blue_min_val:\n",
    "        logging.warning(\"bad frame detected, reason: blue > blue_min_val\")\n",
    "        return False\n",
    "    if red <= green:\n",
    "        logging.warning(\"bad frame detected, reason: red > green\")\n",
    "        return False\n",
    "    if red <= blue:\n",
    "        logging.warning(\"bad frame detected, reason: red > blue\")\n",
    "        return False\n",
    "    if abs(red - green) <= red_green_max_diff:\n",
    "        logging.warning(\"bad frame detected, reason: abs(red - green) > red_green_max_diff\")\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_new_frame(vidcap):\n",
    "    \"\"\"\n",
    "    Reads new video frame and return it.\n",
    "    @param vidcap: Pointer to the video \n",
    "    @return True, Image where there is more frames in the video to be read else False, None \n",
    "    \"\"\"\n",
    "    success, next_image = vidcap.read()\n",
    "    return success, next_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting RGB arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_result(greens, reds, blues, x_value, title=\"\"):\n",
    "    \"\"\"\n",
    "    Generic function to plot graphs.\n",
    "    @param greens: 1D array, greens[i] is the mean of all the green color in frame i \n",
    "    @param reds: 1D array, reds[i] is the mean of all the red color in frame i\n",
    "    @param blues:1D array, blues[i] is the mean of all the blue color in frame i\n",
    "    @param x_value: 1D array for the X- axis\n",
    "    @param title: Title of the plot default is \"\"\n",
    "    @return: None\n",
    "    \"\"\"\n",
    "    logging.info(\"Plotting results ...\" + title)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(x_value, greens, color=\"green\")\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(x_value, reds, color=\"red\")\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(x_value, blues, color=\"blue\")\n",
    "    plt.show()\n",
    "    logging.info(\"Showing result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def filter_channel(channel,fs):\n",
    "    \"\"\"\n",
    "    This method apply filter on a channel between 0.75HZ to 4HZ.\n",
    "    :param channel: Is a signal to apply the filter to.\n",
    "    :param fs: Is the sampling rate of channel.\n",
    "    :return: The filtered channel.\n",
    "    \"\"\"\n",
    "    bh, ah = sig.butter(4, 0.75 / (fs / 2), 'highpass')\n",
    "    bl, al = sig.butter(4, 4 / (fs / 2), 'lowpass')\n",
    "    try:\n",
    "        channel = sig.filtfilt(bh, ah, channel) # applying the filter coefficient on the sig\n",
    "    except:\n",
    "        return None\n",
    "    #channel = np.absolute(channel)\n",
    "    channel_after_filter = sig.filtfilt(bl, al, channel) # applying the filter coefficient on the sig\n",
    "    return channel_after_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get RGB values from a frame\n",
    "#### Check light frame light condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def parse_luminace(red, green, blue):\n",
    "    \"\"\"\n",
    "    Calculate frame's luminace\n",
    "    :param red: Mean value of all pixels in the red channel\n",
    "    :param green: Mean value of all pixels in the green channel\n",
    "    :param blue: Mean value of all pixels in the blue channel\n",
    "    :return: The luminace value\n",
    "    \"\"\"\n",
    "    luminance_level = 0.2126 * red + 0.7152 * green + 0.0722 * blue\n",
    "    return luminance_level\n",
    "\n",
    "def parse_RGB(rois, color_sig):\n",
    "    \"\"\"\n",
    "    Calculate all the mean value of the green channel in ROI.\n",
    "    :param roi: Area that we used to extract HR\n",
    "    :param color_sig: is the the green signal thus far\n",
    "    :return: False in case there is an error, color_sig and luminace value\n",
    "    \"\"\"\n",
    "    for i,r in enumerate(rois):\n",
    "        # extracting RGB colors from the frame\n",
    "        red = r.getchannel(0)\n",
    "        green = r.getchannel(1)\n",
    "        blue = r.getchannel(2)\n",
    "        b_mean,g_mean,r_mean = np.mean(blue),np.mean(green),np.mean(red)\n",
    "        luminance_level = parse_luminace(r_mean, g_mean, b_mean)\n",
    "        if good_frame(b_mean,g_mean,r_mean):\n",
    "            #color_channels = r.reshape(-1, r.shape[-1])\n",
    "            #avg_color = color_channels.mean(axis=0)\n",
    "            color_sig[i].append(g_mean)\n",
    "    return True, color_sig, luminance_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_video_details(video_source):\n",
    "    \"\"\"\n",
    "    Logging current video information\n",
    "    :param video_source: Location of the current video that is being proccessed.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    logging.info(\"\\nInformation on video:\\t\\t\\t\\t\\t\\t\" + str(video_source) +\n",
    "                 \"\\nFPS:\\t\\t\\t\\t\\t\\t\" + str(fps) + \n",
    "                 \"\\nRound FPS:\\t\\t\\t\\t\\t\\t\" + str(round_fps) + \n",
    "                 \"\\nNumber of frames:\\t\\t\\t\\t\" + str(number_of_frames) + \n",
    "                 \"\\nNumber of bad frames:\\t\\t\\t\\t\" + str(bad_frames) + \n",
    "                 \"\\nMax luminanace:\\t\\t\\t\\t\\t\" + str(max_luminance) + \n",
    "                 \"\\nMin luminanace:\\t\\t\\t\\t\\t\" + str(min_luminance) +\n",
    "                 \"\\nMax diff of luminanace between adjacent frames:\\t\" + str(max_diff_luminance_adjacent) +\n",
    "                 \"\\nAvg luminanace:\\t\\t\\t\\t\\t\" + str(avg_luminance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def indexes(y, thres=0.3, min_dist=1, thres_abs=False):\n",
    "    \"\"\"\n",
    "    Finds the peaks in the signal\n",
    "    :param y:  is the signal\n",
    "    :param thres: minimum height of the peaks\n",
    "    :param min_dist: minimum distance between each peek \n",
    "    :param thres_abs: \n",
    "    :return: array of all the peaks\n",
    "    \"\"\"\n",
    "    if isinstance(y, np.ndarray) and np.issubdtype(y.dtype, np.unsignedinteger):\n",
    "        raise ValueError(\"y must be signed\")\n",
    "\n",
    "    if not thres_abs:\n",
    "        thres = thres * (np.max(y) - np.min(y)) + np.min(y)\n",
    "\n",
    "    min_dist = int(min_dist)\n",
    "\n",
    "    # compute first order difference\n",
    "    dy = np.diff(y)\n",
    "\n",
    "    # propagate left and right values successively to fill all plateau pixels (0-value)\n",
    "    zeros, = np.where(dy == 0)\n",
    "\n",
    "    # check if the signal is totally flat\n",
    "    if len(zeros) == len(y) - 1:\n",
    "        return np.array([])\n",
    "\n",
    "    if len(zeros):\n",
    "        # compute first order difference of zero indexes\n",
    "        zeros_diff = np.diff(zeros)\n",
    "        # check when zeros are not chained together\n",
    "        zeros_diff_not_one, = np.add(np.where(zeros_diff != 1), 1)\n",
    "        # make an array of the chained zero indexes\n",
    "        zero_plateaus = np.split(zeros, zeros_diff_not_one)\n",
    "\n",
    "        # fix if leftmost value in dy is zero\n",
    "        if zero_plateaus[0][0] == 0:\n",
    "            dy[zero_plateaus[0]] = dy[zero_plateaus[0][-1] + 1]\n",
    "            zero_plateaus.pop(0)\n",
    "\n",
    "        # fix if rightmost value of dy is zero\n",
    "        if len(zero_plateaus) and zero_plateaus[-1][-1] == len(dy) - 1:\n",
    "            dy[zero_plateaus[-1]] = dy[zero_plateaus[-1][0] - 1]\n",
    "            zero_plateaus.pop(-1)\n",
    "\n",
    "        # for each chain of zero indexes\n",
    "        for plateau in zero_plateaus:\n",
    "            median = np.median(plateau)\n",
    "            # set leftmost values to leftmost non zero values\n",
    "            dy[plateau[plateau < median]] = dy[plateau[0] - 1]\n",
    "            # set rightmost and middle values to rightmost non zero values\n",
    "            dy[plateau[plateau >= median]] = dy[plateau[-1] + 1]\n",
    "\n",
    "    # find the peaks by using the first order difference\n",
    "    peaks = np.where(\n",
    "        (np.hstack([dy, 0.0]) < 0.0)\n",
    "        & (np.hstack([0.0, dy]) > 0.0)\n",
    "        & (np.greater(y, thres))\n",
    "    )[0]\n",
    "\n",
    "    # handle multiple peaks, respecting the minimum distance\n",
    "    if peaks.size > 1 and min_dist > 1:\n",
    "        highest = peaks[np.argsort(y[peaks])][::-1]\n",
    "        rem = np.ones(y.size, dtype=bool)\n",
    "        rem[peaks] = False\n",
    "\n",
    "        for peak in highest:\n",
    "            if not rem[peak]:\n",
    "                sl = slice(max(0, peak - min_dist), peak + min_dist + 1)\n",
    "                rem[sl] = True\n",
    "                rem[peak] = False\n",
    "\n",
    "        peaks = np.arange(y.size)[~rem]\n",
    "\n",
    "    return peaks\n",
    "\n",
    "def print_results(frames_window, window, xlabel, ylabel, change_range, title):\n",
    "    \"\"\"\n",
    "    Generic function that plots signals for a 30 seconds window\n",
    "    @param frames_window: 1D array represent the seconds in the window \n",
    "    @param window:HR 30 seconds signal \n",
    "    @param xlabel: label to be shown in the X - axis\n",
    "    @param ylabel: label to be shown in the Y - axis\n",
    "    @param change_range:\n",
    "    @return: None\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle(title)\n",
    "    ax = fig.subplots()\n",
    "    while len(frames_window) > len(window):\n",
    "        frames_window = frames_window[:-1]\n",
    "    ax.plot(frames_window,window,color ='green')\n",
    "    if change_range:\n",
    "        plt.xlim([0, 5])\n",
    "    # add some data to the signal \n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xscale('linear')\n",
    "    ax.spines['bottom'].set_color('red')\n",
    "    ax.spines['left'].set_color('red')\n",
    "    ax.xaxis.label.set_color('red')\n",
    "    ax.yaxis.label.set_color('red')\n",
    "    ax.tick_params(axis='x', colors='red')\n",
    "    ax.tick_params(axis='y', colors='red')\n",
    "    plt.show()\n",
    "\n",
    "def find_peaks_with_scipy(frames_window, window, round_fps, hr):\n",
    "    \"\"\" \n",
    "    Find all the peaks in window\n",
    "    :param frames_window: 1D array represent the seconds in the window \n",
    "    :param window: HR 30 seconds signal \n",
    "    :param round_fps: round frames per second\n",
    "    :param hr: is the Heart rate \n",
    "    :return: None\n",
    "    \"\"\" \n",
    "    while len(frames_window) > len(window):\n",
    "        frames_window = frames_window[:-1]\n",
    "    peaks, _ = sig.find_peaks(window, distance=round_fps)\n",
    "    plt.plot(frames_window,window,'-go',markerfacecolor='red',markevery=peaks)\n",
    "    plt.title(\"Peaks with scipy\")\n",
    "    plt.show()\n",
    "    logging.info(\"Peaks vector with scipy: \" + str(peaks) + \" num of peaks: \" + str(len(peaks)))\n",
    "\n",
    "def print_peaks(frames_window, window):\n",
    "    \"\"\"\n",
    "    Add a mark to all the peaks in window\n",
    "    :param frames_window: 1D array represent the seconds in the window \n",
    "    :param window: 30 seconds signal \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    while len(frames_window) > len(window):\n",
    "        frames_window = frames_window[:-1]\n",
    "    peaks = indexes(window,min_dist=20)\n",
    "    plt.plot(frames_window,window,'-go',markerfacecolor='red',markevery=peaks)\n",
    "    plt.title(\"Peaks with implemented function\")\n",
    "    plt.show()\n",
    "    logging.info(\"Peaks vector by implemented function: \" + str(frames_window[peaks]) + \" num of peaks: \" + str(len(frames_window[peaks])))\n",
    "  \n",
    "    \n",
    "def find_hr_in_window(green, window_start, round_fps, window_id, window_size):\n",
    "    \"\"\"\n",
    "    Split the signal into windows and find HR in each window\n",
    "    :param green: is the signal of the all video\n",
    "    :param window_start: index of where the window start in green\n",
    "    :param round_fps: round frames per seconds\n",
    "    :param window_id: window number\n",
    "    :param window_size: size of the window - 30 seconds or less if there are not enough frames\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    round_fps = int(round_fps)\n",
    "    if window_start + round_fps * window_size > len(green):\n",
    "        window = green[window_start : ]\n",
    "    else:\n",
    "        window = green[window_start : window_start + round_fps * window_size]\n",
    "    frames_window = np.arange(window.size/round_fps,step= (1/round_fps))\n",
    "    #plot the window:\n",
    "    print_results(frames_window, window, 'X-axis', 'Y-axis', False, \"Green signal\")\n",
    "    # normalize window signal\n",
    "    window = window - np.mean(window)\n",
    "    window = window / np.std(window)\n",
    "    print_results(frames_window, window, 'X-axis', 'Y-axis', False, \"Green signal normalized\")\n",
    "    # filter the signal and plot results\n",
    "    g = filter_channel(window,round_fps)\n",
    "    if g is None: # An error occurred while filtering the signal\n",
    "        return \n",
    "    print_peaks(frames_window, g)\n",
    "    print_results(frames_window, window, 'X-axis', 'Y-axis', False, \"\")\n",
    "    #plot the frequencies:\n",
    "    f, Pxx_den = sig.periodogram(g, round_fps)\n",
    "    print_results(f, Pxx_den, 'Frequency [Hz]', 'PSD [V**2/Hz]', True, \"PSD by Frequency\")\n",
    "    # find the maximum freq.\n",
    "    max_val = Pxx_den.argmax()\n",
    "    logging.info(\"Window \" + str(window_id) +\n",
    "                 \":\\nHighest freq:\" + str(f[max_val]) + \"\\nHeart rate: \" + str(f[max_val]*60))\n",
    "    # find peaks - peak means breath\n",
    "    find_peaks_with_scipy(frames_window, g,round_fps, f[max_val]*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "def eigenvalue_decomposition(mat):\n",
    "    \"\"\"\n",
    "    Apply eigenvalue decomposition of the matrix mat\n",
    "    :param mat: the matrix to apply the decomposition to.\n",
    "    :return: diagonal matrix which consist of the eigen values of mat eigen vectors\n",
    "    \"\"\"\n",
    "    eigen_vals, eigen_vectors = np.linalg.eigh(mat)\n",
    "    d = np.diag(eigen_vals)\n",
    "    return d,eigen_vectors\n",
    "\n",
    "    \n",
    "def multivariate_video_signal(signal):\n",
    "    \"\"\"\n",
    "    Implementation of the algorithm from the paper.\n",
    "    :param signal: is a 2D signal. One dimension the signal from the forehead, and second is from above the upper lip ( 1 dimention for each region in rois)\n",
    "    :return: multivariate PPG as described in the paper.\n",
    "    \"\"\"    \n",
    "     # convert the signal into vector\n",
    "    sig1 = signal[0,:].T\n",
    "    sig2 = signal[1,:].T\n",
    "    n = len(sig1)\n",
    "    wavelet = pywt.Wavelet('sym2') \n",
    "    # apply wavlet decomposition:\n",
    "    c = pywt.wavedec(np.column_stack((sig1,sig2)),wavelet,level = 4,axis = 0)\n",
    "    mcd = MCD(random_state = 0).fit(c[-1]).covariance_ # noise matrix\n",
    "    V,D,VT = np.linalg.svd(mcd) # noise matrix decomposition\n",
    "    lst = []\n",
    "    pca = PCA(1) \n",
    "    comp = StandardScaler().fit_transform(c[0]) # normalize the features\n",
    "    c[0] = comp\n",
    "    comp = pca.fit_transform(comp)\n",
    "    comp = np.column_stack((comp,comp))\n",
    "    lst.append(comp)\n",
    "    # Basis change \n",
    "    for i in range (1,len(c)): # the denoising\n",
    "        x_i = np.dot(c[i],V)\n",
    "        gama_0 = np.sqrt(2*np.log(n)*D[0])\n",
    "        xi_0 = pywt.threshold(x_i[:,0],value= gama_0,mode='soft')\n",
    "        gama_1 = np.sqrt(2*np.log(n)*D[1])\n",
    "        xi_1 = pywt.threshold(x_i[:,1],value= gama_1,mode='soft')\n",
    "        xi = np.column_stack((xi_0,xi_1))\n",
    "        xi = xi@VT\n",
    "        lst.append(xi)\n",
    "    return pywt.waverec(lst,wavelet,axis = 0) # reconstruct 2d array\n",
    "\n",
    "def mvd(color_sig):\n",
    "    \"\"\"\n",
    "    Prefom multivariance denoising\n",
    "    :param color_sig: is the HR signal with noise \n",
    "    :return: Denoise HR rate signal\n",
    "    \"\"\"\n",
    "    ppg_hat = multivariate_video_signal(color_sig)\n",
    "    return ppg_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def detect_hr(video_source):\n",
    "    \"\"\"\n",
    "    Find HR estimation from the signal\n",
    "    :param video_source: path to the video that is being procceesd\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    logging.info(\"\\n=======================\\n\" + video_source + \"\\n=======================\\n\")\n",
    "    color_sig_array = np.asarray(color_sig) # convert the signal to ndarray\n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(color_sig_array[0],\"green\")\n",
    "    sig_mvd = mvd(color_sig_array) # apply the denoising\n",
    "    # filtering the signals\n",
    "    ppg_hat_zero = filter_channel(sig_mvd[:,0].T,round_fps) \n",
    "    ppg_hat_one = filter_channel(sig_mvd[:,1].T,round_fps)\n",
    "    ppg_hat = np.vstack((ppg_hat_zero,ppg_hat_one))\n",
    "    plt.subplot(2,1,2)\n",
    "    green = ppg_hat[0] # choose the signal that was extracted from the subject's forehead.\n",
    "    plt.plot(green,\"red\")\n",
    "    # Split the signal into 30 seconds signals and find HR in each of them\n",
    "    window_start = 0\n",
    "    window_size = 30\n",
    "    window_id = 0\n",
    "    limit = good_frame_number - int(round_fps) * window_size\n",
    "    while window_start < limit :\n",
    "        find_hr_in_window(green, window_start, round_fps, window_id, window_size)\n",
    "        window_start += int(round_fps) * window_size\n",
    "        window_id += 1\n",
    "    if window_start < good_frame_number:\n",
    "        find_hr_in_window(green, window_start, round_fps, window_id, good_frame_number - window_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Main loop - going over all the frames of the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Parsing video:\n",
    "for video_location in video_sources:\n",
    "    color_sig = [[],[]]\n",
    "    heart_rates = []\n",
    "    good_frame_number = 0\n",
    "#     total_frame_number = 180\n",
    "    total_frame_number = 0\n",
    "    logging.info(\"Working on video \" + video_location)\n",
    "    vidcap = cv2.VideoCapture(video_location)\n",
    "    success, image = vidcap.read()\n",
    "    fps = vidcap.get(cv2.CAP_PROP_FPS) # fs == sampling rate\n",
    "    round_fps = np.round(fps)\n",
    "    number_of_frames = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    logging.info(\"Parsing images ...\")\n",
    "    skipped_frames = 0\n",
    "    bad_frames = 0\n",
    "    max_luminance = 0\n",
    "    min_luminance = 200\n",
    "    avg_luminance = 0\n",
    "    perv_luminance = None\n",
    "    max_diff_luminance_adjacent = 0\n",
    "\n",
    "    while success:\n",
    "        if total_frame_number % 100 == 0:\n",
    "            logging.info(\"Parsing frame \" + str(total_frame_number) + \"/\" + str(number_of_frames))\n",
    "        rois = parse_roi(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))  # build image ROI (rois is a tuple contains two regions)\n",
    "        if rois is not None :\n",
    "            try:\n",
    "                is_good_frame,color_sig, luminance_level = parse_RGB(rois, color_sig) # add mean value of the  green channel in the current frame to the signal \n",
    "                if perv_luminance is not None and luminance_level - perv_luminance > max_diff_luminance_adjacent:\n",
    "                    max_diff_luminance_adjacent = luminance_level - perv_luminance\n",
    "                if luminance_level > max_luminance:\n",
    "                    max_luminance = luminance_level\n",
    "                if luminance_level < min_luminance:\n",
    "                    min_luminance = luminance_level\n",
    "                avg_luminance += luminance_level\n",
    "                perv_luminance = luminance_level\n",
    "            except Exception as e:\n",
    "                logging.error(\"failed to get output from parse_RGB!\\nError:\" + str(e))\n",
    "                is_good_frame = False\n",
    "                bad_frames += 1\n",
    "            if is_good_frame:\n",
    "                good_frame_number += 1\n",
    "                #logging.info(\"luminance level: \" + str(luminance_level))\n",
    "        if rois is None:\n",
    "            bad_frames += 1\n",
    "        total_frame_number += 1\n",
    "        success, image = get_new_frame(vidcap)\n",
    "    avg_luminance /= total_frame_number\n",
    "    log_video_details(video_location)\n",
    "    try:\n",
    "        detect_hr(video_location) # Display the HR\n",
    "    except Exception as e:\n",
    "        logging.warning(\"Issue in detecting hr in video: \" + str(e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
