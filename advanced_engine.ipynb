{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# KOIOS\n",
    "* Detecting the heart rate of a patient using signal processing and machine learning\n",
    "* Project number: 87\n",
    "* By Perry Tubul 205874290 and Eyal Golan 204229223"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Table of Contents\n",
    "1. [Advanced engine implementation](#Advanced)\n",
    "2. [Setup](#Setup)\n",
    "3. [Engine](#Engine)\n",
    "4. [HR detection](#hr)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <a class=\"anchor\" id=\"Advanced\">1. Advanced engine implementation</a>\n",
    "Using a similar logic as the basic engine, with the following changes for better results:\n",
    "* Wavelet based Multivariate De-noising -\n",
    "* MTCNN neural network - in order to detect the regions of intrest of a moving patient\n",
    "* luminance detection and manipulation for more accurate heart rate results - used mainly when there are changing or bad\n",
    " lightning conditions"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <a class=\"anchor\" id=\"Setup\">2. Setup</a>\n",
    "#### Initializing the Logger and the MTCNN neural networks, reading the videos location and detecting the OS"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-08-06 10:19:32,501] [INFO] [<module>] [30] : Starting ...\n"
     ]
    }
   ],
   "source": [
    "from facenet_pytorch import MTCNN #for detecting ROI of a moving subject\n",
    "import torch\n",
    "import face_recognition, PIL.Image, PIL.ImageDraw,math\n",
    "import logging\n",
    "import cv2\n",
    "import platform\n",
    "import scipy.signal as sig\n",
    "import os\n",
    "import pywt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.covariance import MinCovDet as MCD\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "\n",
    "FORMAT = '[%(asctime)s] [%(levelname)s] [%(funcName)s] [%(lineno)d] : %(message)s'\n",
    "logging.basicConfig(format=FORMAT, level=logging.INFO)\n",
    "\n",
    "with open(\"input_path.txt\") as input_file:\n",
    "    input_location = input_file.read()\n",
    "pattern = \".avi\"\n",
    "video_sources = []\n",
    "\n",
    "for path, subdirs, files in os.walk(input_location):\n",
    "    for name in files:\n",
    "        if name.endswith(pattern):\n",
    "            video_sources.append(os.path.join(path, name))\n",
    "\n",
    "logging.info(\"Starting ...\")\n",
    "if platform.system() == \"Windows\":\n",
    "    seperator = \"\\\\\"\n",
    "else:\n",
    "    seperator = \"/\"\n",
    "\n",
    "\n",
    "# since MTCNN is a collection of neural nets and other code, the device must be passed\n",
    "# in the following way to enable copying of objects when needed internally\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "mtcnn = MTCNN(keep_all=True, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#%run ./evm_preprocessing.ipynb\n",
    "# video_location = dataset_location + specific_dir + seperator + \"out.avi\"\n",
    "#video_location=\"out2.avi\""
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <a class=\"anchor\" id=\"Engine\">3. Engine</a>\n",
    "## 3.1 ROI detection\n",
    "Detects face landmarks and parsing the ROI"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def detect_face(frame):\n",
    "    \"\"\"\n",
    "    Detect face in a frame\n",
    "    :param frame: A video frame\n",
    "    :return: the location of the face in the picture\n",
    "    \"\"\"\n",
    "    face_locations_handle_motion = mtcnn.detect(frame) # using pre-trained model to find faces\n",
    "    face_location = list() \n",
    "    face_location.append(float(face_locations_handle_motion[0][0][1]))\n",
    "    face_location.append(float(face_locations_handle_motion[0][0][2]))\n",
    "    face_location.append(float(face_locations_handle_motion[0][0][3]))\n",
    "    face_location.append(float(face_locations_handle_motion[0][0][0]))\n",
    "    face_locations = list()\n",
    "    face_locations.append(face_location)\n",
    "    return face_locations\n",
    "\n",
    "\n",
    "def parse_nose_to_top_lip(face_landmarks_list, img, leftest_point, rightest_point):\n",
    "    # finding the second ROI - nose to top lip:\n",
    "    try:\n",
    "        upper_mouth = np.asarray(face_landmarks_list[0]['top_lip']) # top_lip landmarks\n",
    "    except Exception as e:\n",
    "        logging.warning(\"No upper mouth found, \" + str(e))\n",
    "        return None\n",
    "    upper_mouth_min = upper_mouth.min(axis = 0)[1] # The  top - lip upper point.\n",
    "    try:\n",
    "        upper_nose = np.asarray(face_landmarks_list[0]['nose_bridge'])\n",
    "    except Exception as e:\n",
    "        logging.warning(\"No upper nose found, \" + str(e))\n",
    "        return None\n",
    "    upper_nose_min = upper_nose.min(axis = 0)[1]  # noise bridge upper point.\n",
    "    upper_nose_min += upper_mouth_min * 0.1 # improving the noise bridge upper point.\n",
    "    nose_to_upper_lip = img.crop((leftest_point[0], upper_nose_min, rightest_point[0], upper_mouth_min))\n",
    "\n",
    "    return nose_to_upper_lip\n",
    "\n",
    "\n",
    "def parse_forehead(face_landmarks_list, img, diff):\n",
    "    # finding the forehead\n",
    "    try:\n",
    "        right_eyebrow_landmarks = np.asarray(face_landmarks_list[0]['right_eyebrow']) # right eyebrow points.\n",
    "    except Exception as e:\n",
    "        logging.warning(\"No forehead found, \" + str(e))\n",
    "        return None\n",
    "    right_eyebrow_landmarks.sort(axis=0)\n",
    "    rightest_point = right_eyebrow_landmarks[-1] # The most right point of the ROI(according to x).\n",
    "    top_right_eyebrow = right_eyebrow_landmarks.min(axis = 0)[1]\n",
    "    try:\n",
    "        left_eyebrow_landmarks = np.asarray(face_landmarks_list[0]['left_eyebrow'])\n",
    "    except Exception as e:\n",
    "        logging.warning(\"No left eyebrow found, \" + str(e))\n",
    "        return None\n",
    "    left_eyebrow_landmarks.sort(axis=0)\n",
    "    leftest_point = left_eyebrow_landmarks[0] # the most left point of ROI.(according to x)\n",
    "    top_left_eyebrow = left_eyebrow_landmarks.min(axis = 0)[1]\n",
    "    bottom = min(top_right_eyebrow,top_left_eyebrow).item(0) # bottom point of the forehead.\n",
    "    bottom = bottom - (0.05 * bottom) # improve bottom location by 2 percent.\n",
    "    # adding diff to top to make the forehead bigger.\n",
    "    forehead = img.crop((leftest_point[0], leftest_point[1]+diff, rightest_point[0],bottom+10))\n",
    "\n",
    "    return forehead, leftest_point, rightest_point\n",
    "\n",
    "def parse_face_locations(frame, face_location, face_landmarks_list):\n",
    "    img = PIL.Image.fromarray(frame)\n",
    "    top,right,bottom,left = face_location # extract all face square points.\n",
    "    diff = math.floor((top - bottom) * 0.15) # 20 percent of the face len (to add eyebrow top point).\n",
    "\n",
    "    # finding the forehead\n",
    "    forehead, leftest_point, rightest_point = parse_forehead(face_landmarks_list, img, diff)\n",
    "\n",
    "    # finding the second ROI - nose to top lip:\n",
    "    nose_to_upper_lip = parse_nose_to_top_lip(face_landmarks_list, img, leftest_point, rightest_point)\n",
    "\n",
    "    return forehead, nose_to_upper_lip\n",
    "\n",
    "def parse_roi(frame):\n",
    "    \"\"\"\n",
    "    Find a face and its region of interests.  \n",
    "    :param frame: A video frame\n",
    "    :return: None in case of which no face was detected. A tuple in a form of (forehead, nose_to_upper_lip)\n",
    "    forehead is ndarray that represents the subject's forehead, nose_to_upper_lip is ndarray that represents the region\n",
    "    between the upper lip and the nose of the subject. \n",
    "    \"\"\"\n",
    "    face_locations = face_recognition.face_locations(frame,model = 'hog') # detects all the faces in image\n",
    "    face_landmarks_list = face_recognition.face_landmarks(image)\n",
    "    \n",
    "    try:\n",
    "        face_locations = detect_face(frame)\n",
    "    except Exception as e:\n",
    "        logging.error(\"Failure in face detection, error: \" + str(e))\n",
    "        \n",
    "    # iterate through all the faces.\n",
    "    for face_location in face_locations:\n",
    "        forehead, nose_to_upper_lip = parse_face_locations(frame, face_location, face_landmarks_list)\n",
    "        return forehead, nose_to_upper_lip\n",
    "    return None # in case of which no face was detected"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Check for bad frames\n",
    "* Criteria:  red > 95 and green > 40 and blue > 20 and red > green and red > blue\n",
    "* Based on https://arxiv.org/ftp/arxiv/papers/1708/1708.02694.pdf page 5"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "RED_MIN_VALUE = 95\n",
    "GREEN_MIN_VALUE = 40\n",
    "BLUE_MIN_VALUE = 20\n",
    "RED_GREEN_MAX_DIFF = 15\n",
    "def good_frame(blue, green, red):\n",
    "    \"\"\"\n",
    "        Checks the light conditions \n",
    "        :param: blue Is the mean of all the blue pixels in one frame\n",
    "        :param: green Is the mean  of all the green pixels in one frame\n",
    "        :param: red Is the mean of all the red pixels in one frame\n",
    "        :return False 0 meaning the lightning are bad, True when the lightning condition are OK\n",
    "    \"\"\"\n",
    "    if red <= RED_MIN_VALUE:\n",
    "        logging.warning(\"bad frame detected, reason: red > red_min_val\")\n",
    "        return False\n",
    "    if green <= GREEN_MIN_VALUE:\n",
    "        logging.warning(\"bad frame detected, reason: green > green_min_val\")\n",
    "        return False\n",
    "    if blue <= BLUE_MIN_VALUE:\n",
    "        logging.warning(\"bad frame detected, reason: blue > blue_min_val\")\n",
    "        return False\n",
    "    if red <= green:\n",
    "        logging.warning(\"bad frame detected, reason: red > green\")\n",
    "        return False\n",
    "    if red <= blue:\n",
    "        logging.warning(\"bad frame detected, reason: red > blue\")\n",
    "        return False\n",
    "    if abs(red - green) <= RED_GREEN_MAX_DIFF:\n",
    "        logging.warning(\"bad frame detected, reason: abs(red - green) > red_green_max_diff\")\n",
    "        return False\n",
    "\n",
    "    return True"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%s\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def get_new_frame(vidcap):\n",
    "    \"\"\"\n",
    "    Reads new video frame and return it.\n",
    "    :param vidcap: Pointer to the video\n",
    "    :return True, Image where there is more frames in the video to be read else False, None\n",
    "    \"\"\"\n",
    "    success, next_image = vidcap.read()\n",
    "    return success, next_image"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3 Plotting RGB arrays"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def plot_result(greens, reds, blues, x_value, title=\"\"):\n",
    "    \"\"\"\n",
    "    Generic function to plot graphs.\n",
    "    :param greens: 1D array, greens[i] is the mean of all the green color in frame i\n",
    "    :param reds: 1D array, reds[i] is the mean of all the red color in frame i\n",
    "    :param blues:1D array, blues[i] is the mean of all the blue color in frame i\n",
    "    :param x_value: 1D array for the X- axis\n",
    "    :param title: Title of the plot default is \"\"\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    logging.info(\"Plotting results ...\" + title)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(x_value, greens, color=\"green\")\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(x_value, reds, color=\"red\")\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(x_value, blues, color=\"blue\")\n",
    "    plt.show()\n",
    "    logging.info(\"Showing result\")"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4 Channel filtering\n",
    "This method applies a filter on a channel between 0.75HZ to 4HZ  (i.e. 60 bpm to 240 bpm)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def filter_channel(channel,fs):\n",
    "    \"\"\"\n",
    "    This method apply filter on a channel between 0.75HZ to 4HZ.\n",
    "    :param channel: Is a signal to apply the filter to.\n",
    "    :param fs: Is the sampling rate of channel.\n",
    "    :return: The filtered channel.\n",
    "    \"\"\"\n",
    "    bh, ah = sig.butter(4, 0.75 / (fs / 2), 'highpass')\n",
    "    bl, al = sig.butter(4, 4 / (fs / 2), 'lowpass')\n",
    "    try:\n",
    "        channel = sig.filtfilt(bh, ah, channel) # applying the filter coefficient on the signal\n",
    "    except Exception as e:\n",
    "        logging.warning(str(e))\n",
    "        return None\n",
    "    channel_after_filter = sig.filtfilt(bl, al, channel) # applying the filter coefficient on the signal\n",
    "    return channel_after_filter"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.5 Get RGB values from a frame and Check light conditions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def parse_luminance(red, green, blue):\n",
    "    \"\"\"\n",
    "    Calculate frame's luminance\n",
    "    :param red: Mean value of all pixels in the red channel\n",
    "    :param green: Mean value of all pixels in the green channel\n",
    "    :param blue: Mean value of all pixels in the blue channel\n",
    "    :return: The luminace value\n",
    "    \"\"\"\n",
    "    red_weight = 0.2126\n",
    "    green_weight = 0.7152\n",
    "    blue_weight = 0.0722\n",
    "\n",
    "    luminance_level = red_weight * red + green_weight * green + blue_weight * blue\n",
    "    return luminance_level\n",
    "\n",
    "def parse_rgb(rois, color_sig):\n",
    "    \"\"\"\n",
    "    Calculate all the mean value of the green channel in ROI.\n",
    "    :param rois: Area that we used to extract HR\n",
    "    :param color_sig: is the the green signal thus far\n",
    "    :return: False in case there is an error, color_sig and luminace value\n",
    "    \"\"\"\n",
    "    for i,r in enumerate(rois):\n",
    "        # extracting RGB colors from the frame\n",
    "        red = r.getchannel(0)\n",
    "        green = r.getchannel(1)\n",
    "        blue = r.getchannel(2)\n",
    "        b_mean,g_mean,r_mean = np.mean(blue),np.mean(green),np.mean(red)\n",
    "        luminance_level = parse_luminance(r_mean, g_mean, b_mean)\n",
    "        if good_frame(b_mean,g_mean,r_mean):\n",
    "            #color_channels = r.reshape(-1, r.shape[-1])\n",
    "            #avg_color = color_channels.mean(axis=0)\n",
    "            color_sig[i].append(g_mean)\n",
    "    return True, color_sig, luminance_level"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.6 Logging information about the video"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def log_video_details(video_source):\n",
    "    \"\"\"\n",
    "    Logging current video information\n",
    "    :param video_source: Location of the current video that is being processed.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    logging.info(\"\\nInformation on video:\\t\\t\\t\\t\\t\\t\" + str(video_source) +\n",
    "                 \"\\nFPS:\\t\\t\\t\\t\\t\\t\" + str(fps) + \n",
    "                 \"\\nRound FPS:\\t\\t\\t\\t\\t\\t\" + str(round_fps) + \n",
    "                 \"\\nNumber of frames:\\t\\t\\t\\t\" + str(number_of_frames) + \n",
    "                 \"\\nNumber of bad frames:\\t\\t\\t\\t\" + str(bad_frames) + \n",
    "                 \"\\nMax luminanace:\\t\\t\\t\\t\\t\" + str(max_luminance) + \n",
    "                 \"\\nMin luminanace:\\t\\t\\t\\t\\t\" + str(min_luminance) +\n",
    "                 \"\\nMax diff of luminanace between adjacent frames:\\t\" + str(max_diff_luminance_adjacent) +\n",
    "                 \"\\nAvg luminanace:\\t\\t\\t\\t\\t\" + str(avg_luminance))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <a class=\"anchor\" id=\"hr\">4. HR detection</a>\n",
    "## 4.1 helper functions for HR detection - used to find the peaks in a given signal\n",
    "We estimated the heart rate by exploiting the periodicity of the PPG signal by a\n",
    "frequency based voting scheme. The PSD of the multivariate PPG signals was derived,\n",
    "and we empirically selected the frequencies with the highest peak"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def fix_leftmost_value(zero_plateaus, dy):\n",
    "    # fix if leftmost value in dy is zero\n",
    "    if zero_plateaus[0][0] == 0:\n",
    "        dy[zero_plateaus[0]] = dy[zero_plateaus[0][-1] + 1]\n",
    "        zero_plateaus.pop(0)\n",
    "\n",
    "def fix_rightmost_value(zero_plateaus, dy):\n",
    "    # fix if rightmost value of dy is zero\n",
    "    if len(zero_plateaus) and zero_plateaus[-1][-1] == len(dy) - 1:\n",
    "        dy[zero_plateaus[-1]] = dy[zero_plateaus[-1][0] - 1]\n",
    "        zero_plateaus.pop(-1)\n",
    "\n",
    "def handle_zero_plateaus(zeros, dy):\n",
    "    if len(zeros):\n",
    "        # compute first order difference of zero indexes\n",
    "        zeros_diff = np.diff(zeros)\n",
    "        # check when zeros are not chained together\n",
    "        zeros_diff_not_one, = np.add(np.where(zeros_diff != 1), 1)\n",
    "        # make an array of the chained zero indexes\n",
    "        zero_plateaus = np.split(zeros, zeros_diff_not_one)\n",
    "\n",
    "        fix_leftmost_value(zero_plateaus, dy)  # fix if leftmost value in dy is zero\n",
    "        fix_rightmost_value(zero_plateaus, dy) # fix if rightmost value of dy is zero\n",
    "\n",
    "        # for each chain of zero indexes: set leftmost values to leftmost non zero values, set rightmost and middle\n",
    "        # values to rightmost non zero values\n",
    "        set_chain_values(zero_plateaus, dy)\n",
    "\n",
    "\n",
    "def set_chain_values(zero_plateaus, dy):\n",
    "    # for each chain of zero indexes\n",
    "    for plateau in zero_plateaus:\n",
    "        median = np.median(plateau)\n",
    "        # set leftmost values to leftmost non zero values\n",
    "        dy[plateau[plateau < median]] = dy[plateau[0] - 1]\n",
    "        # set rightmost and middle values to rightmost non zero values\n",
    "        dy[plateau[plateau >= median]] = dy[plateau[-1] + 1]\n",
    "\n",
    "def handle_multiple_peaks(peaks, min_dist, y):\n",
    "    # handle multiple peaks, respecting the minimum distance\n",
    "    if peaks.size > 1 and min_dist > 1:\n",
    "        highest = peaks[np.argsort(y[peaks])][::-1]\n",
    "        rem = np.ones(y.size, dtype=bool)\n",
    "        rem[peaks] = False\n",
    "\n",
    "        for peak in highest:\n",
    "            if not rem[peak]:\n",
    "                sl = slice(max(0, peak - min_dist), peak + min_dist + 1)\n",
    "                rem[sl] = True\n",
    "                rem[peak] = False\n",
    "\n",
    "        peaks = np.arange(y.size)[~rem]\n",
    "\n",
    "        return peaks\n",
    "\n",
    "\n",
    "def indexes(y, threshold=0.3, min_dist=1, threshold_abs=False):\n",
    "    \"\"\"\n",
    "    Finds the peaks in the signal\n",
    "    :param y:  is the signal\n",
    "    :param threshold: minimum height of the peaks\n",
    "    :param min_dist: minimum distance between each peek \n",
    "    :param threshold_abs:\n",
    "    :return: array of all the peaks\n",
    "    \"\"\"\n",
    "    if isinstance(y, np.ndarray) and np.issubdtype(y.dtype, np.unsignedinteger):\n",
    "        raise ValueError(\"y must be signed\")\n",
    "\n",
    "    if not threshold_abs:\n",
    "        threshold = threshold * (np.max(y) - np.min(y)) + np.min(y)\n",
    "\n",
    "    min_dist = int(min_dist)\n",
    "    dy = np.diff(y)# compute first order difference\n",
    "    zeros, = np.where(dy == 0) # propagate left and right values successively to fill all plateau pixels (0-value)\n",
    "\n",
    "    # check if the signal is totally flat\n",
    "    if len(zeros) == len(y) - 1:\n",
    "        return np.array([])\n",
    "\n",
    "    # compute first order difference of zero indexes, check when zeros are not chained together, fix values, and for\n",
    "    # each chain of zero indexes: set leftmost values to leftmost non zero values, set rightmost and middle values to\n",
    "    # rightmost non zero values\n",
    "    handle_zero_plateaus(zeros, dy)\n",
    "\n",
    "    # find the peaks by using the first order difference\n",
    "    peaks = np.where((np.hstack([dy, 0.0]) < 0.0) & (np.hstack([0.0, dy]) > 0.0) & (np.greater(y, threshold)))[0]\n",
    "\n",
    "    # handle multiple peaks, respecting the minimum distance\n",
    "    peaks = handle_multiple_peaks(peaks, min_dist, y)\n",
    "\n",
    "    return peaks"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 support functions for plotting the results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def print_results(frames_window, window, xlabel, ylabel, change_range, title):\n",
    "    \"\"\"\n",
    "    Generic function that plots signals for a 30 seconds window\n",
    "    :param frames_window: 1D array represent the seconds in the window\n",
    "    :param window:HR 30 seconds signal\n",
    "    :param xlabel: label to be shown in the X - axis\n",
    "    :param ylabel: label to be shown in the Y - axis\n",
    "    :param change_range:\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle(title)\n",
    "    ax = fig.subplots()\n",
    "    while len(frames_window) > len(window):\n",
    "        frames_window = frames_window[:-1]\n",
    "    ax.plot(frames_window,window,color ='green')\n",
    "    if change_range:\n",
    "        plt.xlim([0, 5])\n",
    "    # add some data to the signal \n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xscale('linear')\n",
    "    ax.spines['bottom'].set_color('red')\n",
    "    ax.spines['left'].set_color('red')\n",
    "    ax.xaxis.label.set_color('red')\n",
    "    ax.yaxis.label.set_color('red')\n",
    "    ax.tick_params(axis='x', colors='red')\n",
    "    ax.tick_params(axis='y', colors='red')\n",
    "    plt.show()\n",
    "\n",
    "def print_peaks_with_scipy(frames_window, window, round_fps, hr):\n",
    "    \"\"\" \n",
    "    Find all the peaks in window\n",
    "    :param frames_window: 1D array represent the seconds in the window \n",
    "    :param window: HR 30 seconds signal \n",
    "    :param round_fps: round frames per second\n",
    "    :param hr: is the Heart rate \n",
    "    :return: None\n",
    "    \"\"\" \n",
    "    while len(frames_window) > len(window):\n",
    "        frames_window = frames_window[:-1]\n",
    "    peaks, _ = sig.find_peaks(window, distance=round_fps)\n",
    "    plt.plot(frames_window,window,'-go',markerfacecolor='red',markevery=peaks)\n",
    "    plt.title(\"Peaks with scipy\")\n",
    "    plt.show()\n",
    "    logging.info(\"Peaks vector with scipy: \" + str(peaks) + \" num of peaks: \" + str(len(peaks)))\n",
    "\n",
    "def print_peaks(frames_window, window):\n",
    "    \"\"\"\n",
    "    Add a mark to all the peaks in window\n",
    "    :param frames_window: 1D array represent the seconds in the window \n",
    "    :param window: 30 seconds signal \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    while len(frames_window) > len(window):\n",
    "        frames_window = frames_window[:-1]\n",
    "    peaks = indexes(window,min_dist=20)\n",
    "    plt.plot(frames_window,window,'-go',markerfacecolor='red',markevery=peaks)\n",
    "    plt.title(\"Peaks with implemented function\")\n",
    "    plt.show()\n",
    "    logging.info(\"Peaks vector by implemented function: \" + str(frames_window[peaks]) + \" num of peaks: \" +\n",
    "                 str(len(frames_window[peaks])))\n",
    "  "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.3 Split the signal into windows and find HR in each window\n",
    "We operate by estimating an average heart rate over a windows of 30 seconds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def find_hr_in_window(green, window_start, round_fps, window_id, window_size):\n",
    "    \"\"\"\n",
    "    Split the signal into windows and find HR in each window\n",
    "    :param green: is the signal of the all video\n",
    "    :param window_start: index of where the window start in green\n",
    "    :param round_fps: round frames per seconds\n",
    "    :param window_id: window number\n",
    "    :param window_size: size of the window - 30 seconds or less if there are not enough frames\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    round_fps = int(round_fps)\n",
    "    if window_start + round_fps * window_size > len(green):\n",
    "        window = green[window_start : ]\n",
    "    else:\n",
    "        window = green[window_start : window_start + round_fps * window_size]\n",
    "    frames_window = np.arange(window.size/round_fps,step= (1/round_fps))\n",
    "    #plot the window:\n",
    "    print_results(frames_window, window, 'X-axis', 'Y-axis', False, \"Green signal\")\n",
    "    # normalize window signal\n",
    "    window = window - np.mean(window)\n",
    "    window = window / np.std(window)\n",
    "    print_results(frames_window, window, 'X-axis', 'Y-axis', False, \"Green signal normalized\")\n",
    "    # filter the signal and plot results\n",
    "    g = filter_channel(window,round_fps)\n",
    "    if g is None: # an error occurred while filtering the signal\n",
    "        return \n",
    "    print_peaks(frames_window, g)\n",
    "    print_results(frames_window, window, 'X-axis', 'Y-axis', False, \"\")\n",
    "    #plot the frequencies:\n",
    "    f, pxx_density = sig.periodogram(g, round_fps)\n",
    "    print_results(f, pxx_density, 'Frequency [Hz]', 'PSD [V**2/Hz]', True, \"PSD by Frequency\")\n",
    "    # find the maximum freq\n",
    "    max_val = pxx_density.argmax()\n",
    "    logging.info(\"Window \" + str(window_id) +\n",
    "                 \":\\nHighest freq:\" + str(f[max_val]) + \"\\nHeart rate: \" + str(f[max_val]*60))\n",
    "    # find peaks - peak means breath\n",
    "    print_peaks_with_scipy(frames_window, g,round_fps, f[max_val] * 60)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.4 DWT - Wavelet based Multivariate De-noising\n",
    "* The de-noising of the multivariate signal is performed by removing the noise elements Îµ by\n",
    "thresholding the noisy signals\n",
    "* The heart beat is shifted to a frequency with lower power in the power spectral distribution (PSD).\n",
    "* We estimated the heart rate by exploiting the periodicity of the PPG signal by a frequency based voting scheme"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "def eigenvalue_decomposition(mat):\n",
    "    \"\"\"\n",
    "    Apply eigenvalue decomposition of the matrix mat\n",
    "    :param mat: the matrix to apply the decomposition to.\n",
    "    :return: diagonal matrix which consist of the eigenvalues of mat eigen vectors\n",
    "    \"\"\"\n",
    "    eigen_vals, eigen_vectors = np.linalg.eigh(mat)\n",
    "    d = np.diag(eigen_vals)\n",
    "    return d,eigen_vectors\n",
    "\n",
    "    \n",
    "def multivariate_video_signal(signal):\n",
    "    \"\"\"\n",
    "    Implementation of the algorithm from the paper.\n",
    "    :param signal: is a 2D signal. One dimension the signal from the forehead, and second is from above the upper lip\n",
    "    (1 dimension for each region in rois)\n",
    "    :return: multivariate PPG as described in the paper.\n",
    "    \"\"\"    \n",
    "    # convert the signal into vector\n",
    "    first_signal = signal[0,:].T\n",
    "    second_signal = signal[1,:].T\n",
    "    first_signal_length = len(first_signal)\n",
    "    wavelet_filter = 'sym2'\n",
    "    wavelet = pywt.Wavelet(wavelet_filter)\n",
    "\n",
    "    # apply wavelet decomposition:\n",
    "    wavelet_decomposition = pywt.wavedec(np.column_stack((first_signal,second_signal)),wavelet,level = 4,axis = 0)\n",
    "    mcd = MCD(random_state = 0).fit(wavelet_decomposition[-1]).covariance_ # noise matrix\n",
    "    V,D,VT = np.linalg.svd(mcd) # noise matrix decomposition\n",
    "    basis_change_list = []\n",
    "    pca = PCA(1) \n",
    "    compatible_transform = StandardScaler().fit_transform(wavelet_decomposition[0]) # normalize the features\n",
    "    wavelet_decomposition[0] = compatible_transform\n",
    "    compatible_transform = pca.fit_transform(compatible_transform)\n",
    "    compatible_transform = np.column_stack((compatible_transform,compatible_transform))\n",
    "    basis_change_list.append(compatible_transform)\n",
    "\n",
    "    # Basis change \n",
    "    for i in range (1, len(wavelet_decomposition)): # the de-noising\n",
    "        x_i = np.dot(wavelet_decomposition[i],V)\n",
    "        gama_0 = np.sqrt(2 * np.log(first_signal_length) * D[0])\n",
    "        xi_0 = pywt.threshold(x_i[:,0],value= gama_0,mode='soft')\n",
    "        gama_1 = np.sqrt(2 * np.log(first_signal_length) * D[1])\n",
    "        xi_1 = pywt.threshold(x_i[:,1],value= gama_1,mode='soft')\n",
    "        xi = np.column_stack((xi_0,xi_1))\n",
    "        xi = xi@VT\n",
    "        basis_change_list.append(xi)\n",
    "    return pywt.waverec(basis_change_list,wavelet,axis = 0) # reconstruct 2d array\n",
    "\n",
    "def mvd(color_sig):\n",
    "    \"\"\"\n",
    "    Perform multivariate de-noising\n",
    "    :param color_sig: is the HR signal with noise \n",
    "    :return: De-noise HR rate signal\n",
    "    \"\"\"\n",
    "    ppg_hat = multivariate_video_signal(color_sig)\n",
    "    return ppg_hat\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.5 Find HR estimation from the signal"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def detect_hr(video_source):\n",
    "    \"\"\"\n",
    "    Find HR estimation from the signal\n",
    "    :param video_source: path to the video that is being procceesd\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    logging.info(\"\\n=======================\\n\" + video_source + \"\\n=======================\\n\")\n",
    "    color_sig_array = np.asarray(color_sig) # convert the signal to ndarray\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(color_sig_array[0],\"green\")\n",
    "    sig_mvd = mvd(color_sig_array) # apply the de-noising\n",
    "    # filtering the signals\n",
    "    ppg_hat_zero = filter_channel(sig_mvd[:,0].T,round_fps) \n",
    "    ppg_hat_one = filter_channel(sig_mvd[:,1].T,round_fps)\n",
    "    ppg_hat = np.vstack((ppg_hat_zero,ppg_hat_one))\n",
    "    plt.subplot(2, 1, 2)\n",
    "    green = ppg_hat[0] # choose the signal that was extracted from the subject's forehead.\n",
    "    plt.plot(green,\"red\")\n",
    "    # Split the signal into 30 seconds signals and find HR in each of them\n",
    "    window_start = 0\n",
    "    window_size = 30\n",
    "    window_id = 0\n",
    "    limit = good_frame_number - int(round_fps) * window_size\n",
    "    while window_start < limit :\n",
    "        find_hr_in_window(green, window_start, round_fps, window_id, window_size)\n",
    "        window_start += int(round_fps) * window_size\n",
    "        window_id += 1\n",
    "    if window_start < good_frame_number:\n",
    "        find_hr_in_window(green, window_start, round_fps, window_id, good_frame_number - window_start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.6 Main loop - going over all the frames of the video"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Parsing video:\n",
    "for video_location in video_sources:\n",
    "    color_sig = [[],[]]\n",
    "    heart_rates = []\n",
    "    good_frame_number = 0\n",
    "    total_frame_number = 0\n",
    "    logging.info(\"Working on video \" + video_location)\n",
    "    vidcap = cv2.VideoCapture(video_location)\n",
    "    success, image = vidcap.read()\n",
    "    fps = vidcap.get(cv2.CAP_PROP_FPS) # fs == sampling rate\n",
    "    round_fps = np.round(fps)\n",
    "    number_of_frames = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    logging.info(\"Parsing images ...\")\n",
    "    skipped_frames = 0\n",
    "    bad_frames = 0\n",
    "    max_luminance = 0\n",
    "    min_luminance = 200\n",
    "    avg_luminance = 0\n",
    "    perv_luminance = None\n",
    "    max_diff_luminance_adjacent = 0\n",
    "\n",
    "    while success:\n",
    "        if total_frame_number % 100 == 0:\n",
    "            logging.info(\"Parsing frame \" + str(total_frame_number) + \"/\" + str(number_of_frames))\n",
    "        # build image ROI (rois is a tuple contains two regions)\n",
    "        rois = parse_roi(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        if rois is not None :\n",
    "            try:\n",
    "                # add mean value of the  green channel in the current frame to the signal\n",
    "                is_good_frame,color_sig, luminance_level = parse_rgb(rois, color_sig)\n",
    "                if perv_luminance is not None and luminance_level - perv_luminance > max_diff_luminance_adjacent:\n",
    "                    max_diff_luminance_adjacent = luminance_level - perv_luminance\n",
    "                if luminance_level > max_luminance:\n",
    "                    max_luminance = luminance_level\n",
    "                if luminance_level < min_luminance:\n",
    "                    min_luminance = luminance_level\n",
    "                avg_luminance += luminance_level\n",
    "                perv_luminance = luminance_level\n",
    "            except Exception as e:\n",
    "                logging.error(\"failed to get output from parse_RGB!\\nError:\" + str(e))\n",
    "                is_good_frame = False\n",
    "                bad_frames += 1\n",
    "            if is_good_frame:\n",
    "                good_frame_number += 1\n",
    "        if rois is None:\n",
    "            bad_frames += 1\n",
    "        total_frame_number += 1\n",
    "        success, image = get_new_frame(vidcap)\n",
    "    avg_luminance /= total_frame_number\n",
    "    log_video_details(video_location)\n",
    "    try:\n",
    "        detect_hr(video_location) # detect the heart rate\n",
    "    except Exception as e:\n",
    "        logging.warning(\"Issue in detecting hr in video: \" + str(e))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-08-06 10:19:32,770] [INFO] [<module>] [7] : Working on video C:\\Users\\eyalg\\Downloads\\dataset\\subject4\\vid.avi\n",
      "[2021-08-06 10:19:32,821] [INFO] [<module>] [13] : Parsing images ...\n",
      "[2021-08-06 10:19:32,821] [INFO] [<module>] [24] : Parsing frame 0/2028.0\n",
      "C:\\Users\\eyalg\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "[2021-08-06 10:20:11,354] [INFO] [<module>] [24] : Parsing frame 100/2028.0\n",
      "[2021-08-06 10:20:53,344] [INFO] [<module>] [24] : Parsing frame 200/2028.0\n",
      "[2021-08-06 10:21:35,448] [INFO] [<module>] [24] : Parsing frame 300/2028.0\n",
      "[2021-08-06 10:22:17,167] [INFO] [<module>] [24] : Parsing frame 400/2028.0\n",
      "[2021-08-06 10:22:58,825] [INFO] [<module>] [24] : Parsing frame 500/2028.0\n",
      "[2021-08-06 10:23:41,520] [INFO] [<module>] [24] : Parsing frame 600/2028.0\n",
      "[2021-08-06 10:24:23,794] [INFO] [<module>] [24] : Parsing frame 700/2028.0\n",
      "[2021-08-06 10:25:05,610] [INFO] [<module>] [24] : Parsing frame 800/2028.0\n",
      "[2021-08-06 10:25:47,010] [INFO] [<module>] [24] : Parsing frame 900/2028.0\n",
      "[2021-08-06 10:26:28,517] [INFO] [<module>] [24] : Parsing frame 1000/2028.0\n",
      "[2021-08-06 10:27:09,808] [INFO] [<module>] [24] : Parsing frame 1100/2028.0\n",
      "[2021-08-06 10:27:51,835] [INFO] [<module>] [24] : Parsing frame 1200/2028.0\n",
      "[2021-08-06 10:28:33,899] [INFO] [<module>] [24] : Parsing frame 1300/2028.0\n",
      "[2021-08-06 10:29:15,233] [INFO] [<module>] [24] : Parsing frame 1400/2028.0\n",
      "[2021-08-06 10:29:59,890] [INFO] [<module>] [24] : Parsing frame 1500/2028.0\n"
     ]
    }
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-ca99968e",
   "language": "python",
   "display_name": "PyCharm (Koios)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}