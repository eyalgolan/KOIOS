{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Basic engine implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-03-29 00:17:26,993] [INFO] [<module>] [14] : Starting ...\n"
     ]
    }
   ],
   "source": [
    "import face_recognition, PIL.Image, PIL.ImageDraw,math\n",
    "import numpy as np\n",
    "import logging\n",
    "import cv2\n",
    "import platform\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import scipy.signal as sig\n",
    "\n",
    "FORMAT = '[%(asctime)s] [%(levelname)s] [%(funcName)s] [%(lineno)d] : %(message)s'\n",
    "logging.basicConfig(format=FORMAT, level=logging.INFO)\n",
    "\n",
    "logging.info(\"Starting ...\")\n",
    "if platform.system() == \"Windows\":\n",
    "    seperator = \"\\\\\"\n",
    "else:\n",
    "    seperator = \"/\"\n",
    "\n",
    "dir = \"perry-all-2\"\n",
    "# should be a parameter of the engine\n",
    "dataset_location = \"..\" + seperator + \"dataset\" + seperator + \"good_sync\" + seperator\n",
    "specific_dir = dir\n",
    "video_location = dataset_location + specific_dir + seperator + \"test.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## running evm pre-processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spatial filtering...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "corr_dn() missing 2 required positional arguments: 'start' and 'stop'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-2-3a7c4431e93a>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mget_ipython\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmagic\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'run ./evm_preprocessing.ipynb'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;31m#hpyer params here were taken from the matlab implementation but might need to be changed (there were several options and I took the face option)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0mamplify_spatial_Gdown_temporal_ideal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvideo_location\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"/\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m50\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m50\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0;36m60\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m60\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0;36m60\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m30\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0mvideo_location\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdataset_location\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mspecific_dir\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mseperator\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m\"out.avi\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-2-9708a1231c8c>\u001B[0m in \u001B[0;36mamplify_spatial_Gdown_temporal_ideal\u001B[0;34m(vid_file, out_file, alpha, level, fl, fh, fs, chrom_attenuation)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Spatial filtering...'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m     \u001B[0mgdown_stack\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbuild_gdown_stack\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvid_file\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mstart_index\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mend_index\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mlevel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     18\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Finished'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-2-3cecb74a489b>\u001B[0m in \u001B[0;36mbuild_gdown_stack\u001B[0;34m(vid_file, start_index, end_index, level)\u001B[0m\n\u001B[1;32m     10\u001B[0m     \u001B[0mframe\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrgb2ntsc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 12\u001B[0;31m     \u001B[0mblurred\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mblur_dn_clr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mlevel\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# need to implement this\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m     \u001B[0mgdown_stack\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzeros\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mend_index\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart_index\u001B[0m \u001B[0;34m+\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mblurred\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mblurred\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mblurred\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-2-9e2290ffd701>\u001B[0m in \u001B[0;36mblur_dn_clr\u001B[0;34m(im, nlevels, filt)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mblur_dn_clr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mim\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mnlevels\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mfilt\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'binom5'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m     \u001B[0mtmp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mblur_dn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mim\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mnlevels\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mfilt\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m     \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzeros\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtmp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtemp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mim\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mout\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtmp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-2-988e5bf36226>\u001B[0m in \u001B[0;36mblur_dn\u001B[0;34m(im, nlevs, filt)\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mnlevs\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m         \u001B[0mim\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mblur_dn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mim\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mnlevs\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mfilt\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mnlevs\u001B[0m\u001B[0;34m>=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-2-988e5bf36226>\u001B[0m in \u001B[0;36mblur_dn\u001B[0;34m(im, nlevs, filt)\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mnlevs\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m         \u001B[0mim\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mblur_dn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mim\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mnlevs\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mfilt\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mnlevs\u001B[0m\u001B[0;34m>=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-2-988e5bf36226>\u001B[0m in \u001B[0;36mblur_dn\u001B[0;34m(im, nlevs, filt)\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mnlevs\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m         \u001B[0mim\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mblur_dn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mim\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mnlevs\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mfilt\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mnlevs\u001B[0m\u001B[0;34m>=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-2-988e5bf36226>\u001B[0m in \u001B[0;36mblur_dn\u001B[0;34m(im, nlevs, filt)\u001B[0m\n\u001B[1;32m     26\u001B[0m             \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcorr_dn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mres\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mfilt\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'reflect1'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masarray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 28\u001B[0;31m             \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcorr_dn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mim\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mfilt\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'reflect1'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masarray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     29\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     30\u001B[0m         \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mim\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: corr_dn() missing 2 required positional arguments: 'start' and 'stop'"
     ]
    }
   ],
   "source": [
    "%run ./evm_preprocessing.ipynb\n",
    "#hpyer params here were taken from the matlab implementation but might need to be changed (there were several options and I took the face option)\n",
    "amplify_spatial_Gdown_temporal_ideal(video_location, \"/\", 50,4,50/60,60/60,30, 1)\n",
    "video_location = dataset_location + specific_dir + seperator + \"out.avi\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detecting the face landmarks and parsing the ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_roi(frame):\n",
    "    # image = face_recognition.load_image_file(frame) # read image.\n",
    "    face_locations = face_recognition.face_locations(frame,model = 'hog') # detects all the faces in image\n",
    "    face_landmarks_list = face_recognition.face_landmarks(image)\n",
    "    \n",
    "    # iterate through all the faces.\n",
    "    for face_location in face_locations:\n",
    "        img = PIL.Image.fromarray(frame)\n",
    "        top,right,bottom,left = face_location # extract all face square points.\n",
    "        diff = math.floor((top - bottom) * 0.15) # 20 percent of the face len (toadd eyebrow top point).\n",
    "        \n",
    "        # finding the forehead\n",
    "        right_eyebrow_landmarks = np.asarray(face_landmarks_list[0]['right_eyebrow']) # right eyebrow points.\n",
    "        right_eyebrow_landmarks.sort(axis=0)\n",
    "        rightest_point = right_eyebrow_landmarks[-1] # The most right point of the ROI(according to x).\n",
    "        top_right_eyebrow = right_eyebrow_landmarks.min(axis = 0)[1]\n",
    "        left_eyebrow_landmarks = np.asarray(face_landmarks_list[0]['left_eyebrow'])\n",
    "        left_eyebrow_landmarks.sort(axis=0)\n",
    "        leftest_point = left_eyebrow_landmarks[0] # the most left point of ROI.(according to x)\n",
    "        top_left_eyebrow = left_eyebrow_landmarks.min(axis = 0)[1]\n",
    "        bottom = min(top_right_eyebrow,top_left_eyebrow).item(0) # bottom point of the forehead.\n",
    "        bottom = bottom - (0.05 * bottom) # improve bottom location by 2 percent.\n",
    "        forehead = img.crop((leftest_point[0], leftest_point[1]+diff, rightest_point[0],bottom+10)) # adding diff to top to make the forehead bigger.\n",
    "\n",
    "        # finding the second ROI:\n",
    "        upper_mouth = np.asarray(face_landmarks_list[0]['top_lip']) # top_lip landmarks\n",
    "        upper_mouth_min = upper_mouth.min(axis = 0)[1] # The  top - lip upper point.\n",
    "        upper_nose = np.asarray(face_landmarks_list[0]['nose_bridge'])\n",
    "        upper_nose_min = upper_nose.min(axis = 0)[1]  # noise bridge upper point.\n",
    "        upper_nose_min += upper_mouth_min * 0.1 # improving the noise bridge upper point.\n",
    "        nose_to_upper_lip = img.crop((leftest_point[0], upper_nose_min, rightest_point[0], upper_mouth_min))\n",
    "\n",
    "        return forehead, nose_to_upper_lip\n",
    "    return None # in case of which no face was detected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for bad frames\n",
    "##### R > 95 and G > 40 and B > 20 and R > G and R > B\n",
    "##### Based on https://arxiv.org/ftp/arxiv/papers/1708/1708.02694.pdf page 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%s\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "red_min_val = 95\n",
    "green_min_val = 40\n",
    "blue_min_val = 20\n",
    "red_green_max_diff = 15\n",
    "def bad_frame(blue, green, red):\n",
    "    if red <= red_min_val:\n",
    "        logging.warning(\"bad frame detected, reason: red > red_min_val\")\n",
    "        return False\n",
    "    if green <= green_min_val:\n",
    "        logging.warning(\"bad frame detected, reason: green > green_min_val\")\n",
    "        return False\n",
    "    if blue <= blue_min_val:\n",
    "        logging.warning(\"bad frame detected, reason: blue > blue_min_val\")\n",
    "        return False\n",
    "    if red <= green:\n",
    "        logging.warning(\"bad frame detected, reason: red > green\")\n",
    "        return False\n",
    "    if red <= blue:\n",
    "        logging.warning(\"bad frame detected, reason: red > blue\")\n",
    "        return False\n",
    "    if abs(red - green) <= red_green_max_diff:\n",
    "        logging.warning(\"bad frame detected, reason: abs(red - green) > red_green_max_diff\")\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_new_frame(vidcap):\n",
    "    success, next_image = vidcap.read()\n",
    "    return success, next_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting RGB arrays results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_result(greens, reds, blues, x_value, title=\"\"):\n",
    "    logging.info(\"Plotting results ...\" + title)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(x_value, greens, color=\"green\")\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(x_value, reds, color=\"red\")\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(x_value, blues, color=\"blue\")\n",
    "    plt.show()\n",
    "    logging.info(\"Showing result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def filter_channel(channel,fs):\n",
    "    \"\"\"\n",
    "    This method apply filter on a channel between 0.75HZ to 4HZ.\n",
    "    :param channel: Is a signal to apply the filter to.\n",
    "    :param fs: Is the sampling rate of channel.\n",
    "    :return: The filtered channel.\n",
    "    \"\"\"\n",
    "    bh, ah = sig.butter(4, 0.75 / (fs / 2), 'highpass')\n",
    "    bl, al = sig.butter(4, 4 / (fs / 2), 'lowpass')\n",
    "    channel = sig.filtfilt(bh, ah, channel) # applying the filter coefficient on the sig\n",
    "    channel = np.absolute(channel)\n",
    "    channel_after_filter = sig.filtfilt(bl, al, channel) # applying the filter coefficient on the sig\n",
    "    return channel_after_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting RGB values from a frame and adding them to arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_RGB(roi, color_sig):\n",
    "    \"\"\"\n",
    "    Parses an image to its RGB channels\n",
    "    :param image: the image to be parsed\n",
    "    :param vidcap:\n",
    "    :param greens: array containing green channel values\n",
    "    :param blues: array containing blue channel values\n",
    "    :param reds: array containing red channel values\n",
    "    :param frame_number - is the number of the frame of the video.\n",
    "    :return: a flag indicating if there is a next image, and the next image\n",
    "    \"\"\"\n",
    "#     plt.imshow(roi)\n",
    "#     plt.show()\n",
    "    try:\n",
    "        roi = cv2.cvtColor(roi, cv2.COLOR_RGB2BGR)\n",
    "    except:\n",
    "        return False, color_sig\n",
    "    new_blue,new_green,new_red = cv2.split(roi)\n",
    "    b_mean,g_mean,r_mean = np.mean(new_blue),np.mean(new_green),np.mean(new_red)\n",
    "    if not bad_frame(b_mean,g_mean,r_mean):\n",
    "        color_channels = roi.reshape(-1, roi.shape[-1])\n",
    "        avg_color = color_channels.mean(axis=0) \n",
    "        color_sig.append(avg_color)\n",
    "        return True, color_sig\n",
    "    return False, color_sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main loop - going over all the frames of the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "color_sig = []\n",
    "heart_rates = []\n",
    "good_frame_number = 0\n",
    "total_frame_number = 0\n",
    "\n",
    "# Parsing video:\n",
    "logging.info(\"Working on video \" + video_location)\n",
    "vidcap = cv2.VideoCapture(video_location)\n",
    "success, image = vidcap.read()\n",
    "fps = vidcap.get(cv2.CAP_PROP_FPS) # fs == sampling rate\n",
    "round_fps = np.round(fps)\n",
    "number_of_frames = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "logging.info(\"Parsing images ...\")\n",
    "while success:\n",
    "    logging.info(\"parsing frame \" + str(total_frame_number) + \"/\" + str(number_of_frames))\n",
    "    rois = parse_roi(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))  # build image ROI (rois is a tuple contains two regions)\n",
    "    if rois is not None : \n",
    "        roi = np.asarray(rois[0]) # Just the forehead\n",
    "        is_good_frame,color_sig = parse_RGB(roi, color_sig)\n",
    "        if is_good_frame:\n",
    "            good_frame_number += 1\n",
    "    total_frame_number += 1\n",
    "    success, image = get_new_frame(vidcap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_hr_in_window(green, window_start, round_fps, window_id, window_size):\n",
    "    round_fps = int(round_fps)\n",
    "    if window_start + round_fps * window_size > len(green):\n",
    "        window = green[window_start : ]\n",
    "    else:\n",
    "        window = green[window_start : window_start + round_fps * window_size]\n",
    "    window_sig = np.arange(window.size/round_fps,step= (1/30))\n",
    "    ax = plt.axes()\n",
    "    ax.set_facecolor(\"white\")\n",
    "    plt.plot(window_sig,window,color ='green')\n",
    "    plt.show()\n",
    "\n",
    "    window = window - np.mean(window)\n",
    "    window = window / np.std(window)\n",
    "    plt.plot(window_sig,window,color ='green')\n",
    "    plt.show()\n",
    "\n",
    "    g = filter_channel(window,round_fps)\n",
    "\n",
    "    plt.plot(window_sig,g,color ='green')\n",
    "    plt.show()\n",
    "\n",
    "    f, Pxx_den = sig.periodogram(g, round_fps)\n",
    "\n",
    "    plt.semilogy(f, Pxx_den)\n",
    "    plt.ylim([1e-7, 1e2])\n",
    "    plt.xlabel('frequency [Hz]')\n",
    "    plt.ylabel('PSD [V**2/Hz]')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    max_val = Pxx_den.argmax()\n",
    "    logging.info(\"Heart rate of window \" + str(window_id) +\n",
    "                 \": \" + str(f[max_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "color_sig_array = np.asarray(color_sig)\n",
    "red = color_sig_array[:,0]\n",
    "green = color_sig_array[:,1]\n",
    "blue = color_sig_array[:,2]\n",
    "\n",
    "window_start = 0\n",
    "window_size = 30\n",
    "window_id = 1\n",
    "while window_start < round_fps * window_size:\n",
    "    find_hr_in_window(green, window_start, round_fps, window_id, window_size)\n",
    "    window_start += int(round_fps) * window_size\n",
    "    window_id += 1\n",
    "find_hr_in_window(green, window_start, round_fps, window_id, window_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Pxx_den\n",
    "ind = np.argsort(a)\n",
    "max_ind = ind[-5:]\n",
    "print(f[max_ind])\n",
    "print(\"HR for this video is: \" +str(f[max_ind].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if good_frame_number != greens.size: # TO BE REVIEW BY EYAL, POSSIBLE BUG FIX HERE!\n",
    "    frame_number = greens.size\n",
    "axis = np.arange((good_frame_number / round_fps), step=(1 / 30)) # axis is Time\n",
    "plot_result(greens, reds, blues, axis, \"All 3 channels\") # original signals\n",
    "# apply filtering on all the channels:\n",
    "green_buttered = filter_channel(greens,round_fps)\n",
    "red_buttered = filter_channel(reds,round_fps)\n",
    "blue_buttered = filter_channel(blues,round_fps)\n",
    "# plotting the channels after apllying the filter\n",
    "plot_result(green_buttered, red_buttered, blue_buttered, axis, \"After Filter\") # after filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if good_frame_number != greens.size: # TO BE REVIEW BY EYAL, POSSIBLE BUG FIX HERE!\n",
    "    frame_number = greens.size\n",
    "axis = np.arange((good_frame_number / round_fps), step=(1 / 30)) # axis is Time\n",
    "plot_result(greens, reds, blues, axis, \"All 3 channels\") # original signals\n",
    "# apply filtering on all the channels:\n",
    "green_buttered = filter_channel(greens,round_fps)\n",
    "red_buttered = filter_channel(reds,round_fps)\n",
    "blue_buttered = filter_channel(blues,round_fps)\n",
    "# plotting the channels after apllying the filter\n",
    "plot_result(green_buttered, red_buttered, blue_buttered, axis, \"After Filter\") # after filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PSD estimation using 'Welch' or 'Periodogram'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f, Pxx_den = sig.welch(green_buttered, round_fps,'flattop', 1024, scaling='spectrum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Plot final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(f, Pxx_den)\n",
    "#plt.semilogy(f, Pxx_den)\n",
    "plt.ylim([1e-7, 1e2])\n",
    "plt.xlabel('frequency [Hz]')\n",
    "plt.ylabel('PSD [V**2/Hz]')\n",
    "plt.show()\n",
    "# periodogram method:\n",
    "f, Pxx_den = sig.periodogram(green_buttered, round_fps)\n",
    "plt.semilogy(f, Pxx_den)\n",
    "plt.ylim([1e-7, 1e2])\n",
    "plt.xlabel('frequency [Hz]')\n",
    "plt.ylabel('PSD [V**2/Hz]')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}